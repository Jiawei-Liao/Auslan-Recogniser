{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":52950,"databundleVersionId":5973250,"sourceType":"competition"},{"sourceId":6156851,"sourceType":"datasetVersion","datasetId":3531756},{"sourceId":9485508,"sourceType":"datasetVersion","datasetId":5768310},{"sourceId":9484921,"sourceType":"datasetVersion","datasetId":5769927},{"sourceId":9485365,"sourceType":"datasetVersion","datasetId":5770267},{"sourceId":6147567,"sourceType":"datasetVersion","datasetId":3376501}],"dockerImageVersionId":30529,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Move CTC_TPU for local use\nfrom shutil import copyfile\ncopyfile(src = \"/kaggle/input/ctc-tpu/CTC_TPU.py\", dst = \"/kaggle/working//CTC_TPU.py\")\n\nfrom CTC_TPU import classic_ctc_loss","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:40:28.145779Z","iopub.execute_input":"2024-09-26T09:40:28.146412Z","iopub.status.idle":"2024-09-26T09:40:59.960567Z","shell.execute_reply.started":"2024-09-26T09:40:28.146366Z","shell.execute_reply":"2024-09-26T09:40:59.959179Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"D0926 09:40:54.458725470    1909 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0926 09:40:54.458751198    1909 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0926 09:40:54.458754616    1909 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0926 09:40:54.458757390    1909 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0926 09:40:54.458759703    1909 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0926 09:40:54.458763294    1909 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0926 09:40:54.458765662    1909 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0926 09:40:54.458767928    1909 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0926 09:40:54.458770136    1909 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0926 09:40:54.458772340    1909 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0926 09:40:54.458774622    1909 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0926 09:40:54.458776954    1909 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0926 09:40:54.458779290    1909 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0926 09:40:54.458781541    1909 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0926 09:40:54.459001772    1909 ev_epoll1_linux.cc:122]               grpc epoll fd: 64\nD0926 09:40:54.459014021    1909 ev_posix.cc:144]                      Using polling engine: epoll1\nD0926 09:40:54.459035519    1909 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0926 09:40:54.459528274    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0926 09:40:54.459542496    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0926 09:40:54.459546553    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0926 09:40:54.459549612    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0926 09:40:54.459552754    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0926 09:40:54.459555832    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0926 09:40:54.459564032    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0926 09:40:54.459598111    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0926 09:40:54.459630449    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0926 09:40:54.459648745    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0926 09:40:54.459652672    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0926 09:40:54.459655771    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0926 09:40:54.459663397    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0926 09:40:54.459666585    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0926 09:40:54.459669655    1909 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0926 09:40:54.459673914    1909 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0926 09:40:54.463590267    1909 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0926 09:40:54.478284837    1997 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0926 09:40:54.484578479    1997 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-09-26T09:40:54.484563128+00:00\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q tensorflow-addons\n!pip install -q git+https://github.com/hoyso48/tf-utils@main\n!pip install -q Levenshtein\n!pip install -q keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:40:59.962251Z","iopub.execute_input":"2024-09-26T09:40:59.962825Z","iopub.status.idle":"2024-09-26T09:41:16.412463Z","shell.execute_reply.started":"2024-09-26T09:40:59.962796Z","shell.execute_reply":"2024-09-26T09:41:16.411255Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras_nlp\nimport tensorflow.keras.mixed_precision as mixed_precision\n\nfrom tf_utils.schedules import OneCycleLR, ListedLR\nfrom tf_utils.callbacks import Snapshot, SWA\nfrom tf_utils.learners import FGM, AWP\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom tqdm.autonotebook import tqdm\nimport sklearn\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport glob\nimport datetime\n\nfrom Levenshtein import distance\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:41:16.413743Z","iopub.execute_input":"2024-09-26T09:41:16.413996Z","iopub.status.idle":"2024-09-26T09:41:17.766115Z","shell.execute_reply.started":"2024-09-26T09:41:16.413972Z","shell.execute_reply":"2024-09-26T09:41:17.765160Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n/tmp/ipykernel_1909/864907894.py:15: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow Version: 2.12.0\nPython Version: 3.8.17 (default, Jul  4 2023, 06:27:59) \n[GCC 12.2.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Seed all random number generators\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# Accelerator settings\ndef get_strategy(device='TPU'):\n    if \"TPU\" in device:\n        tpu = 'local' if device=='TPU-VM' else None\n        print(\"connecting to TPU...\")\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        IS_TPU = True\n\n    if device == \"GPU\"  or device==\"CPU\":\n        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n        if ngpu>1:\n            print(\"Using multi GPU\")\n            strategy = tf.distribute.MirroredStrategy()\n        elif ngpu==1:\n            print(\"Using single GPU\")\n            strategy = tf.distribute.get_strategy()\n        else:\n            print(\"Using CPU\")\n            strategy = tf.distribute.get_strategy()\n        IS_TPU = False\n\n    if device == \"GPU\":\n        print(\"Num GPUs Available: \", ngpu)\n\n    AUTO     = tf.data.experimental.AUTOTUNE\n    REPLICAS = strategy.num_replicas_in_sync\n    print(f'REPLICAS: {REPLICAS}')\n\n    return strategy, REPLICAS, IS_TPU\n\nSTRATEGY, N_REPLICAS, IS_TPU = get_strategy('TPU-VM')","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:41:17.768385Z","iopub.execute_input":"2024-09-26T09:41:17.768902Z","iopub.status.idle":"2024-09-26T09:41:22.523128Z","shell.execute_reply.started":"2024-09-26T09:41:17.768872Z","shell.execute_reply":"2024-09-26T09:41:22.522290Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"connecting to TPU...\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialise tfrecord files\nTRAIN_FILENAMES = tf.io.gfile.glob('/kaggle/input/ausslfr-5fold/*.tfrecords')\n\nprint(len(TRAIN_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T09:58:07.119082Z","iopub.execute_input":"2024-09-26T09:58:07.119494Z","iopub.status.idle":"2024-09-26T09:58:07.129212Z","shell.execute_reply.started":"2024-09-26T09:58:07.119464Z","shell.execute_reply":"2024-09-26T09:58:07.128319Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"9\n","output_type":"stream"}]},{"cell_type":"code","source":"# Label dictionary\nimport json\nfrom IPython.display import display\n\nwith open('/kaggle/input/australian-fingerspelling/character_to_prediction_index.json') as json_file:\n    CHAR_TO_NUM = json.load(json_file)\nNUM_TO_CHAR = dict([(y+1,x) for x,y in CHAR_TO_NUM.items()] )\nNUM_TO_CHAR[60] = 'S'\nNUM_TO_CHAR[61] = 'E'\nNUM_TO_CHAR[0] = 'P'\n\nTABLE = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(NUM_TO_CHAR.values()),\n        values=list(NUM_TO_CHAR.keys()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\n# Train data frame\ntrain_df = pd.read_csv('/kaggle/input/australian-fingerspelling/train.csv')\ndisplay(train_df.head())\ndisplay(train_df.info())","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:00:47.969403Z","iopub.execute_input":"2024-09-26T10:00:47.969750Z","iopub.status.idle":"2024-09-26T10:00:48.004542Z","shell.execute_reply.started":"2024-09-26T10:00:47.969724Z","shell.execute_reply":"2024-09-26T10:00:48.003580Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"                    path  file_id  sequence_id  participant_id  \\\n0  landmarks/101.parquet      101        10101               1   \n1  landmarks/101.parquet      101        10104               1   \n2  landmarks/101.parquet      101        10105               1   \n3  landmarks/101.parquet      101        10106               1   \n4  landmarks/101.parquet      101        10107               1   \n\n                       phrase  \n0  abcdefghijklmnopqrstuvwxyz  \n1                  antecedent  \n2                 antifascist  \n3                   cooperate  \n4                    describe  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>file_id</th>\n      <th>sequence_id</th>\n      <th>participant_id</th>\n      <th>phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10101</td>\n      <td>1</td>\n      <td>abcdefghijklmnopqrstuvwxyz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10104</td>\n      <td>1</td>\n      <td>antecedent</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10105</td>\n      <td>1</td>\n      <td>antifascist</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10106</td>\n      <td>1</td>\n      <td>cooperate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10107</td>\n      <td>1</td>\n      <td>describe</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3102 entries, 0 to 3101\nData columns (total 5 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   path            3102 non-null   object\n 1   file_id         3102 non-null   int64 \n 2   sequence_id     3102 non-null   int64 \n 3   participant_id  3102 non-null   int64 \n 4   phrase          3102 non-null   object\ndtypes: int64(3), object(2)\nmemory usage: 121.3+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"code","source":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename.split('/')[-1]).group(1)) for filename in filenames]\n    return np.sum(n)\nprint(count_data_items(TRAIN_FILENAMES), len(train_df))\nassert count_data_items(TRAIN_FILENAMES) == len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:01:09.344598Z","iopub.execute_input":"2024-09-26T10:01:09.344927Z","iopub.status.idle":"2024-09-26T10:01:09.350701Z","shell.execute_reply.started":"2024-09-26T10:01:09.344902Z","shell.execute_reply":"2024-09-26T10:01:09.349900Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"3102 3102\n","output_type":"stream"}]},{"cell_type":"code","source":"#for the lip_lr function. LEFT[i] is matching with RIGHT[i](i.e LEFT[i](x) == -RIGHT[i](x)).\n#computed from https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model.obj\n\nLEFT = [\n         248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n         265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n         282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n         299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n         316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332,\n         333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n         367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n         384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n         401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n         418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n         435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,  #LFACE\n         468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, #LHAND\n         493, 494, 495, 497, 499, 501, 503, 505, 507, 509, 511, 513, #LPOSE\n         515, 517, 519, 521, #LLEG\n         ]\n\nRIGHT = [\n         3, 7, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n         39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n         60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n         81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102,\n         103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n         121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n         139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158,\n         159, 160, 161, 162, 163, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179,\n         180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 201,\n         202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, #RFACE\n        522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, #RHAND\n        490, 491, 492, 496, 498, 500, 502, 504, 506, 508, 510, 512, #RPOSE\n        514, 516, 518, 520, #RLEG\n        ]\n\nCENTRE = [\n          0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 94, 151, 152, 164, 168, 175, 195, 197, 199, 200, #FACE\n          489, #POSE\n          ]\n\nprint(len(LEFT+RIGHT+CENTRE))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:01:12.676926Z","iopub.execute_input":"2024-09-26T10:01:12.677808Z","iopub.status.idle":"2024-09-26T10:01:12.698490Z","shell.execute_reply.started":"2024-09-26T10:01:12.677774Z","shell.execute_reply":"2024-09-26T10:01:12.697607Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"543\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialise variables for training\nROWS_PER_FRAME = 543\nMAX_LEN = 384\nCROP_LEN = MAX_LEN\nNUM_CLASSES  = len(NUM_TO_CHAR.values()) #62\nPAD = -100.\n\nLHAND = np.arange(468, 489).tolist()\nRHAND = np.arange(522, 543).tolist()\nPOINT_LANDMARKS = list(range(543))\n\nNUM_NODES = len(POINT_LANDMARKS)\nCHANNELS = 3*NUM_NODES\n\nprint(NUM_NODES)\nprint(CHANNELS)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:01:15.586999Z","iopub.execute_input":"2024-09-26T10:01:15.587801Z","iopub.status.idle":"2024-09-26T10:01:15.593485Z","shell.execute_reply.started":"2024-09-26T10:01:15.587769Z","shell.execute_reply":"2024-09-26T10:01:15.592662Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"543\n1629\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Function Definitions","metadata":{}},{"cell_type":"code","source":"def preprocess_phrase(phrase, table=TABLE):\n    phrase = tf.strings.join(['S', phrase, 'E']) #'S'+ phrase + 'E'\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    return phrase\n\ndef interp1d_(x, target_len, method='random'):\n    length = tf.shape(x)[1]\n    target_len = tf.maximum(1,target_len)\n    if method == 'random':\n        if tf.random.uniform(()) < 0.33:\n            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n        else:\n            if tf.random.uniform(()) < 0.5:\n                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n            else:\n                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n    else:\n        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n    return x\n\ndef tf_nan_mean(x, axis=0, keepdims=False):\n    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n\ndef tf_nan_std(x, center=None, axis=0, keepdims=False):\n    if center is None:\n        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n    d = x - center\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n\ndef is_left_handed(x):\n    lhand = tf.gather(x, LHAND, axis=1)\n    rhand = tf.gather(x, RHAND, axis=1)\n    lhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(lhand), tf.int32))\n    rhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(rhand), tf.int32))\n    return lhand_nans < rhand_nans\n\ndef flip_lr(x, left=LEFT, right=RIGHT):\n    x,y,z = tf.unstack(x, axis=-1)\n    x = 1-x\n    new_x = tf.stack([x,y,z], -1)\n    new_x = tf.transpose(new_x, [1,0,2])\n    l_x = tf.gather(new_x, left, axis=0)\n    r_x = tf.gather(new_x, right, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(left)[...,None], r_x)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(right)[...,None], l_x)\n    new_x = tf.transpose(new_x, [1,0,2])\n    return new_x\n\nclass Preprocess(tf.keras.layers.Layer):\n    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.point_landmarks = point_landmarks\n\n    def call(self, inputs):\n        # if tf.rank(inputs) == 3:\n        #     x = inputs[None,...]\n        # else:\n        #     x = inputs\n        x = inputs\n        x = filter_nans_tf(x)\n        x = tf.cond(is_left_handed(x), lambda:flip_lr(x), lambda:x)\n        x = x[None,...]\n\n        if self.max_len is not None:\n            x = x[:,:self.max_len]\n        length = tf.shape(x)[1]\n\n        mean = tf_nan_mean(tf.gather(x, self.point_landmarks, axis=2), axis=[1,2], keepdims=True)\n        mean = tf.where(tf.math.is_nan(mean), tf.constant([0.5,0.5,0.],x.dtype), mean)\n        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n\n        x = (x - mean)/std\n\n        x = tf.concat([\n            tf.reshape(x, (-1,length,3*len(self.point_landmarks))),\n            # tf.reshape(dx, (-1,length,3*len(self.point_landmarks))),\n        ], axis = -1)\n\n        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:01:17.625608Z","iopub.execute_input":"2024-09-26T10:01:17.626383Z","iopub.status.idle":"2024-09-26T10:01:17.646300Z","shell.execute_reply.started":"2024-09-26T10:01:17.626352Z","shell.execute_reply":"2024-09-26T10:01:17.645431Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def decode_tfrec(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'coordinates': tf.io.FixedLenFeature([], tf.string),\n        'phrase_encoded': tf.io.VarLenFeature(dtype=tf.int64),\n        'phrase': tf.io.FixedLenFeature([], tf.string),\n    })\n    out = {}\n    out['coordinates']  = tf.transpose(tf.reshape(tf.io.decode_raw(features['coordinates'], tf.float32), (-1,3,ROWS_PER_FRAME)), (0,2,1))\n    out['phrase'] = features['phrase']\n    return out\n\ndef filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n    x = tf.boolean_mask(x, mask, axis=0)\n    return x\n\ndef preprocess(x, augment=False, max_len=MAX_LEN):\n    coord = x['coordinates']\n    if augment:\n        coord = augment_fn(coord, max_len=max_len)\n    coord = tf.ensure_shape(coord, (None,ROWS_PER_FRAME,3))\n\n    inp = tf.cast(Preprocess(max_len=max_len)(coord)[0],tf.float32)\n    tar = preprocess_phrase(x['phrase'])\n\n    return inp, tar\n\ndef augment_phrase(phrase):\n    phrase = keras_nlp.layers.MaskedLMMaskGenerator(NUM_CLASSES-2,\n                                              mask_selection_rate=0.2,\n                                              mask_token_id=0,\n                                              mask_token_rate=0,\n                                              random_token_rate=1,\n                                              unselectable_token_ids=[0,60,61])(phrase)['token_ids']\n    return phrase\n\ndef is_empty(*args):\n    return tf.shape(args[0])[0] > 1\n\ndef resample(x, rate=(0.8,1.2)):\n    rate = tf.random.uniform((), rate[0], rate[1])\n    length = tf.shape(x)[0]\n    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n    new_x = interp1d_(x, new_size)\n    return new_x\n\ndef spatial_random_affine(xyz,\n    scale  = (0.8,1.2),\n    shear = (-0.15,0.15),\n    shift  = (-0.1,0.1),\n    degree = (-30,30),\n):\n    center = tf.constant([0.5,0.5])\n    if scale is not None:\n        scale = tf.random.uniform((),*scale)\n        xyz = scale*xyz\n\n    if shear is not None:\n        xy = xyz[...,:2]\n        z = xyz[...,2:]\n        shear_x = shear_y = tf.random.uniform((),*shear)\n        if tf.random.uniform(()) < 0.5:\n            shear_x = 0.\n        else:\n            shear_y = 0.\n        shear_mat = tf.identity([\n            [1.,shear_x],\n            [shear_y,1.]\n        ])\n        xy = xy @ shear_mat\n        center = center + [shear_y, shear_x]\n        xyz = tf.concat([xy,z], axis=-1)\n\n    if degree is not None:\n        xy = xyz[...,:2]\n        z = xyz[...,2:]\n        xy -= center\n        degree = tf.random.uniform((),*degree)\n        radian = degree/180*np.pi\n        c = tf.math.cos(radian)\n        s = tf.math.sin(radian)\n        rotate_mat = tf.identity([\n            [c,s],\n            [-s, c],\n        ])\n        xy = xy @ rotate_mat\n        xy = xy + center\n        xyz = tf.concat([xy,z], axis=-1)\n\n    if shift is not None:\n        shift = tf.random.uniform((),*shift)\n        xyz = xyz + shift\n\n    return xyz\n\ndef temporal_crop(x, length=MAX_LEN):\n    l = tf.shape(x)[0]\n    offset = tf.random.uniform((), 0, tf.clip_by_value(l-length,1,length), dtype=tf.int32)\n    x = x[offset:offset+length]\n    return x\n\ndef temporal_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n    l = tf.shape(x)[0]\n    mask_size = tf.random.uniform((), *size)\n    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l-mask_size,1,l), dtype=tf.int32)\n    x = tf.tensor_scatter_nd_update(x,tf.range(mask_offset, mask_offset+mask_size)[...,None],tf.fill([mask_size,543,3],mask_value))\n    return x\n\ndef spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n    mask_offset_y = tf.random.uniform(())\n    mask_offset_x = tf.random.uniform(())\n    mask_size = tf.random.uniform((), *size)\n    mask_x = (mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size)\n    mask_y = (mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size)\n    mask = mask_x & mask_y\n    x = tf.where(mask[...,None], mask_value, x)\n    return x\n\ndef augment_fn(x, always=False, max_len=None):\n    if tf.random.uniform(())<0.8 or always:\n        x = resample(x, (0.5,1.5))\n    # if tf.random.uniform(())<0.5 or always:\n    #     x = flip_lr(x)\n    # if max_len is not None:\n    #     x = temporal_crop(x, max_len)\n    if tf.random.uniform(())<0.75 or always:\n        x = spatial_random_affine(x)\n    # if tf.random.uniform(())<0.5 or always:\n    #     x = temporal_mask(x)\n    if tf.random.uniform(())<0.5 or always:\n        x = spatial_mask(x)\n    return x\n\ndef get_tfrec_dataset(tfrecords, batch_size=64, max_len=128, target_len=64, teacher_forcing=True, drop_remainder=False, train=False, augment=False, shuffle=False, repeat=False):\n    # Initialize dataset with TFRecords\n    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n    ds = ds.map(lambda x: preprocess(x, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n\n    if train:\n        ds = ds.filter(is_empty)\n\n    if teacher_forcing:\n        ds = ds.map(lambda x,y:((x,y[:-1]),(y[1:],y[1:-1])), tf.data.AUTOTUNE)\n        if augment:\n            ds = ds.map(lambda x,y:((x[0],augment_phrase(x[1])),y), tf.data.AUTOTUNE)\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(shuffle)\n        options = tf.data.Options()\n        options.experimental_deterministic = (False)\n        ds = ds.with_options(options)\n\n    if batch_size:\n        if teacher_forcing:\n            ds = ds.padded_batch(batch_size, padding_values=((PAD,0),(0,0)), padded_shapes=(([max_len,CHANNELS],[target_len,]),([target_len,],[target_len,])), drop_remainder=drop_remainder)\n        else:\n            ds = ds.padded_batch(batch_size, padding_values=(PAD,0), padded_shapes=([max_len,CHANNELS],[target_len,]), drop_remainder=drop_remainder)\n\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:01:21.077594Z","iopub.execute_input":"2024-09-26T10:01:21.078372Z","iopub.status.idle":"2024-09-26T10:01:21.108702Z","shell.execute_reply.started":"2024-09-26T10:01:21.078339Z","shell.execute_reply":"2024-09-26T10:01:21.107771Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.ops.gen_dataset_ops import filter_dataset_eager_fallback\nclass ECA(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=5, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.kernel_size = kernel_size\n        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n\n    def call(self, inputs, mask=None):\n        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n        nn = tf.expand_dims(nn, -1)\n        nn = self.conv(nn)\n        nn = tf.squeeze(nn, -1)\n        nn = tf.nn.sigmoid(nn)\n        nn = nn[:,None,:]\n        return inputs * nn\n\nclass MaskingDWConv1D(tf.keras.layers.Layer):\n    '''\n    masked DW1Dconv with strides>1, padding=same.\n    NOTE: padded(masked) frames should always be at the beginning or end of the input sequence.\n    '''\n    def __init__(self, kernel_size, strides=1,\n        dilation_rate=1,\n        padding='same',\n        use_bias=False,\n        kernel_initializer='glorot_uniform',**kwargs):\n        super().__init__(**kwargs)\n        assert padding == 'same' or padding == 'causal'\n        self.strides = strides\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.use_bias = use_bias\n        self.padding = padding\n        self.conv = tf.keras.layers.DepthwiseConv1D(\n                            kernel_size,\n                            strides=strides,\n                            dilation_rate=dilation_rate,\n                            padding=padding,\n                            use_bias=use_bias,\n                            kernel_initializer=kernel_initializer)\n        self.supports_masking = True\n\n    def compute_mask(self, inputs, mask=None):\n      if mask is not None:\n        if self.strides > 1:\n          mask = mask[:,::self.strides]\n      return mask\n\n    def call(self, inputs, mask=None):\n        x = inputs\n        if mask is not None:\n            x = tf.where(mask[...,None], x, tf.constant(0., dtype=x.dtype))\n        x = self.conv(x)\n        return x\n\ndef Conv1DBlock(channel_size,\n          kernel_size,\n          dilation_rate=1,\n          strides=1,\n          drop_rate=0.0,\n          expand_ratio=2,\n          activation='swish',\n          name=None):\n    '''\n    efficient conv1d block, @hoyso48\n    '''\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"conv1dblock\"))\n    # Expansion phase\n    def apply(inputs):\n        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n        channels_expand = channels_in * expand_ratio\n\n        skip = inputs\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'pre_bn')(inputs)\n\n        x = tf.keras.layers.Dense(\n            channels_expand,\n            use_bias=True,\n            activation=activation,\n            name=name + '_expand_conv')(x)\n\n        # Depthwise Convolution\n        x = MaskingDWConv1D(kernel_size,\n            dilation_rate=dilation_rate,\n            strides=strides,\n            use_bias=False,\n            name=name + '_dwconv')(x)\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'conv_bn')(x)\n\n        x = ECA()(x)\n\n        x = tf.keras.layers.Dense(\n            channel_size,\n            use_bias=True,\n            name=name + '_project_conv')(x)\n\n        if drop_rate > 0:\n            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n\n        if (channels_in == channel_size) and (strides == 1):\n            x = tf.keras.layers.add([x, skip], name=name + '_add')\n        return x\n\n    return apply","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:01:48.924117Z","iopub.execute_input":"2024-09-26T10:01:48.924499Z","iopub.status.idle":"2024-09-26T10:01:48.941082Z","shell.execute_reply.started":"2024-09-26T10:01:48.924471Z","shell.execute_reply":"2024-09-26T10:01:48.940312Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class PosEmbedding(tf.keras.layers.Layer):\n    def __init__(self, dim=64, max_len=64, **kwargs):\n        super().__init__(**kwargs)\n        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=dim)\n        self.supports_masking = True\n\n    def call(self, x, positions=None):\n        if positions is None:\n            maxlen = tf.shape(x)[1]\n            positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.num_heads = num_heads\n        self.q = tf.keras.layers.Dense(dim, use_bias=False)\n        self.k = tf.keras.layers.Dense(dim, use_bias=False)\n        self.v = tf.keras.layers.Dense(dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n\n    def get_causal_mask(self, q, k):\n        q_len = tf.shape(q)[1]\n        k_len = tf.shape(k)[1]\n        i = tf.range(q_len)[:, None]\n        j = tf.range(k_len)\n        mask = i >= j\n        mask = tf.reshape(mask, (q_len, k_len))\n        return mask\n\n    def merge_input_state(self, input, state, layer):\n        if input is not None and state is not None:\n            return tf.keras.layers.Concatenate(axis=1)([state, layer(input)])\n        elif input is not None and state is None:\n            return layer(input)\n        elif input is None and state is not None:\n            return state\n        else:\n            raise ValueError\n\n    def call(self, q, k=None, v=None, key_state=None, value_state=None, return_states=False, use_causal_mask=False):\n        q = self.q(q)\n        k = self.merge_input_state(k, key_state, self.k)\n        v = self.merge_input_state(v, value_state, self.v)\n        mask = getattr(k, '_keras_mask', None) # we only consider mask from the 'key' here.\n        if mask is not None:\n            mask = mask[:,None,None,:]\n        if use_causal_mask:\n            if mask is not None:\n                mask = tf.logical_and(mask, self.get_causal_mask(q,k)[None,None,:,:])\n            else:\n                mask = self.get_causal_mask(q,k)[None,None,:,:]\n        q_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(q))\n        k_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(k))\n        v_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(v))\n        attn = tf.matmul(q_, k_, transpose_b=True) * self.scale\n\n        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n        attn = self.drop1(attn)\n\n        x = attn @ v_\n        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n        x = self.proj(x)\n        if return_states:\n            return x, k, v\n        else:\n            return x\n\ndef TransformerDecoderBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0., activation='swish', name=None):\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"transformerdecoderblock\"))\n    def apply(q,k,v):\n        x = q\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_self_attn')(x,x,x,use_causal_mask=True)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop1')(x)\n        x = tf.keras.layers.Add(name=name + '_add1')([q, x])\n        attn_out1 = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn2')(x)\n        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_cross_attn')(x,k,v)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop2')(x)\n        x = tf.keras.layers.Add(name=name + '_add2')([attn_out1, x])\n        attn_out2 = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn3')(x)\n        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop3')(x)\n        x = tf.keras.layers.Add(name=name + '_add3')([attn_out2, x])\n        return x\n    return apply\n\n\nclass MultiHeadSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.num_heads = num_heads\n        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        qkv = self.qkv(inputs)\n        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n\n        if mask is not None:\n            mask = mask[:, None, None, :]\n\n        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n        attn = self.drop1(attn)\n\n        x = attn @ v\n        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n        x = self.proj(x)\n        return x\n\ndef TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish', name=None):\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"transformerblock\"))\n    def apply(inputs):\n        x = inputs\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_mhsa')(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop1')(x)\n        x = tf.keras.layers.Add(name=name + '_add1')([inputs, x])\n        attn_out = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'bn2')(x)\n        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop2')(x)\n        x = tf.keras.layers.Add(name=name + '_add2')([attn_out, x])\n        return x\n    return apply","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:03.249892Z","iopub.execute_input":"2024-09-26T10:02:03.250213Z","iopub.status.idle":"2024-09-26T10:02:03.286169Z","shell.execute_reply.started":"2024-09-26T10:02:03.250186Z","shell.execute_reply":"2024-09-26T10:02:03.285059Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class CTCLoss(tf.keras.losses.Loss):\n    def __init__(self, blank_index=0, input_padding_value=0., target_padding_value=0, **kwargs):\n        super().__init__(**kwargs)\n        self.blank_index = blank_index\n        self.input_padding_value = input_padding_value\n        self.target_padding_value = target_padding_value\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.int32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=tf.int32)\n        label_length = y_true != tf.cast(self.target_padding_value, tf.int32)\n        label_length = tf.reduce_sum(tf.cast(label_length, tf.int32), axis=1, keepdims=False) #(B,)\n        mask = getattr(y_pred, '_keras_mask', None)\n        if mask is not None:\n            input_length = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n        else:\n            input_length = tf.cast(tf.shape(y_pred)[1], dtype=tf.int32)\n            input_length = input_length * tf.ones(shape=(batch_len,), dtype=tf.int32)\n\n        # loss = tf.nn.ctc_loss(y_true, y_pred, label_length=label_length, logit_length=input_length, blank_index=0, logits_time_major=False)\n        loss = classic_ctc_loss(y_true, y_pred, label_length=label_length, logit_length=input_length, blank_index=0) #only for the kaggle TPU\n\n        loss = tf.reduce_mean(loss)\n\n        return loss\n\nclass MaskedSCCE(tf.keras.losses.Loss):\n    def __init__(self, num_classes=NUM_CLASSES, from_logits=True, label_smoothing=0.25, **kwargs):\n        super().__init__(**kwargs)\n        self.num_classes = num_classes\n        self.label_smoothing=label_smoothing\n        self.from_logits = from_logits\n\n    def call(self, y_true, y_pred):\n        mask = y_true!=0\n        N = tf.shape(y_true)[0]\n        y_pred = tf.cast(y_pred, tf.float32)\n        y_true = tf.one_hot(y_true, self.num_classes, axis=-1, dtype=tf.float32)\n        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=self.from_logits, label_smoothing=self.label_smoothing)\n        loss = tf.where(mask, loss, tf.constant(0, dtype=tf.float32))\n        loss = tf.reduce_sum(loss)\n        loss = loss / tf.cast(N, tf.float32)\n        return loss\n\nclass Accuracy(tf.keras.metrics.Metric):\n    def __init__(self, **kwargs):\n        super(Accuracy, self).__init__(name=f'acc', **kwargs)\n        self.acc = tf.keras.metrics.SparseCategoricalAccuracy()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n        mask = y_true != 0\n        y_true = tf.boolean_mask(y_true, mask)\n        y_pred = tf.boolean_mask(y_pred, mask)\n        self.acc.update_state(y_true, y_pred)\n\n    def result(self):\n        return self.acc.result()\n\n    def reset_state(self):\n        self.acc.reset_state()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:07.747906Z","iopub.execute_input":"2024-09-26T10:02:07.748712Z","iopub.status.idle":"2024-09-26T10:02:07.763250Z","shell.execute_reply.started":"2024-09-26T10:02:07.748678Z","shell.execute_reply":"2024-09-26T10:02:07.762342Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class GreedyDecoder(tf.keras.layers.Layer):\n    def __init__(self, model, max_output_length=64, sos_token_idx=60, eos_token_idx=61, pad_token_idx=0, **kwargs):\n        super().__init__(**kwargs)\n        self.model = model\n        self.encoder = self.model.get_layer('encoder')\n        self.decoder = self.model.get_layer('att_decoder')\n        self.inference_module = self.model.get_layer('att_decoder')\n        self.max_output_length = max_output_length\n        self.sos_token_idx = sos_token_idx\n        self.eos_token_idx = eos_token_idx\n        self.pad_token_idx = pad_token_idx\n\n    def call(self, batch_x):\n        encoder_out = self.encoder(batch_x)\n\n        time = tf.constant(0, dtype=tf.int32)\n        predictions = tf.ones((tf.shape(batch_x)[0],1), dtype=tf.int32) * self.sos_token_idx\n        pad = tf.ones((tf.shape(batch_x)[0],), dtype=tf.int32) * self.pad_token_idx\n        init = True\n\n        def condition(_time, _predictions):\n            return tf.logical_and(_time < self.max_output_length, tf.logical_not(tf.reduce_all(tf.reduce_any(_predictions==self.eos_token_idx, axis=1))))\n\n        def body(_time, _predictions):\n            out = self.inference_module([_predictions, encoder_out])\n            pred_curr = tf.where(tf.reduce_any(_predictions==self.eos_token_idx, axis=1), [self.pad_token_idx], tf.argmax(out[:,-1], axis=-1, output_type=tf.int32))\n            _predictions = tf.concat([_predictions, pred_curr[...,None]], axis=1)\n            return _time+1, _predictions\n\n        _, predictions = tf.while_loop(condition, body, loop_vars=[time, predictions])\n        return predictions[:,1:]\n\nclass KerasCTCDecoder(tf.keras.layers.Layer):\n    def __init__(self, model, greedy=True, beam_width=100, from_logits=True, **kwargs):\n        super().__init__(**kwargs)\n        self.model = model\n        self.greedy = greedy\n        self.beam_width = beam_width\n        self.from_logits = from_logits\n        self.encoder = self.model.get_layer('encoder')\n        self.ctc_decoder = self.model.get_layer('ctc_decoder')\n\n    def call(self, batch_x):\n        encoder_out = self.encoder(batch_x)\n        input_length = tf.reduce_sum(tf.cast(encoder_out._keras_mask, tf.int32), axis=1)\n        predictions = self.ctc_decoder(encoder_out)\n        if not self.greedy and self.from_logits:\n            predictions = tf.nn.softmax(predictions, axis=-1)\n        predictions = tf.keras.backend.ctc_decode(tf.cast(predictions, tf.float32), input_length=input_length, greedy=self.greedy, beam_width=self.beam_width)[0][0]\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:10.623834Z","iopub.execute_input":"2024-09-26T10:02:10.624764Z","iopub.status.idle":"2024-09-26T10:02:10.638679Z","shell.execute_reply.started":"2024-09-26T10:02:10.624728Z","shell.execute_reply":"2024-09-26T10:02:10.637811Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def make_predictions(recognizer, ds):\n    results = []\n    for batch in tqdm(ds):\n        result = recognizer(batch[0][0])\n        results.append(num_to_char(result.numpy()))\n    results = np.array([item for sublist in results for item in sublist])\n    return results\n\ndef num_to_char(list_of_nums, n2c_dict=NUM_TO_CHAR):\n    def n_to_c(x):\n        return [n2c_dict[a] for a in x if a!=-1]\n    char_list = [''.join(n_to_c(x)).replace('P','').replace('S','').replace('E','') for x in list_of_nums]\n    return np.array(char_list, dtype='str')\n\ndef extract_labels(ds):\n    labels = [num_to_char(x[1][0].numpy()) for x in ds]\n    labels = np.array([item for sublist in labels for item in sublist])\n    return labels\n\nfrom Levenshtein import distance\ndef competition_metric(true, pred):\n    #true: list of strings, ground truths\n    #pred: list of strings, predictions\n    D = sum([distance(x,y) for x,y in zip(true, pred)])\n    N = len(''.join(true))\n    return max((N-D)/N, 0.), D/len(true)\n\ndef display(labels, preds):\n    for target,prediction in zip(labels, preds):\n        print(f\"Target    : {target}\")\n        print(f\"Prediction: {prediction}\")\n        print(\"-\" * 100)\n    return\n\ndef evaluate(model, ds, labels=None, display_index='random', num_display=5):\n    if labels is None:\n        labels = extract_labels(ds)\n    preds = make_predictions(model, ds)\n    score, mean_dist = competition_metric(labels, preds)\n    num_display = min(len(labels), num_display)\n    if display_index=='random':\n        if num_display:\n            idxs = np.random.choice(range(len(labels)),num_display,replace=False)\n            display(labels[idxs], preds[idxs])\n    elif display_index=='init':\n        if num_display:\n            display(labels[:num_display], preds[:num_display])\n    elif isinstance(display_index, list):\n        if display_index:\n            display(labels[display_index], preds[display_index])\n    else:\n        pass\n    print(f'Score: {score:0.4f}')\n    print(f'mean_dist: {mean_dist:0.4f}')\n    # return labels, preds, score\n    del preds, score\n    return\n\nclass Eval(tf.keras.callbacks.Callback):\n    def __init__(self,recognizer,ds,labels=None,eval_epochs=[],display_index='random',num_display=5):\n        super().__init__()\n        self.recognizer = recognizer\n        self.ds = ds\n        self.labels = labels\n        self.eval_epochs = eval_epochs\n        self.display_index = display_index\n        self.num_display = num_display\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch in self.eval_epochs and self.ds is not None: # your custom condition\n            evaluate(self.recognizer, self.ds, self.labels, self.display_index, self.num_display)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:13.118247Z","iopub.execute_input":"2024-09-26T10:02:13.119155Z","iopub.status.idle":"2024-09-26T10:02:13.134530Z","shell.execute_reply.started":"2024-09-26T10:02:13.119113Z","shell.execute_reply":"2024-09-26T10:02:13.133634Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"class AWP(tf.keras.Model):\n    def __init__(self, *args, lr=0.1, eps=1e-6, start_step=0, exclude=[], **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lr = lr\n        self.eps = eps\n        self.start_step = start_step\n        self.exclude = exclude\n\n    def compute_perturbation(self, param, param_gradient):\n        grad = tf.zeros_like(param) + param_gradient\n        #delta = tf.math.divide_no_nan(self.lr * grad * tf.norm(param), tf.norm(grad) + self.eps) #original implemenation from the paper\n        delta = tf.math.divide_no_nan(self.lr * grad, tf.norm(grad) + self.eps)\n        return delta\n\n    def train_step_awp(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        params = self.trainable_variables\n        params_gradients = tape.gradient(loss, self.trainable_variables)\n\n        for i in range(len(params_gradients)):\n            if not any(s in params[i].name for s in self.exclude):\n                delta = self.compute_perturbation(params[i], params_gradients[i])\n                self.trainable_variables[i].assign_add(delta)\n\n        with tf.GradientTape() as tape2:\n            y_pred = self(x, training=True)\n            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n            if hasattr(self.optimizer, 'get_scaled_loss'):\n                new_loss = self.optimizer.get_scaled_loss(new_loss)\n\n        gradients = tape2.gradient(new_loss, self.trainable_variables)\n        if hasattr(self.optimizer, 'get_unscaled_gradients'):\n            gradients =  self.optimizer.get_unscaled_gradients(gradients)\n\n        for i in range(len(params_gradients)):\n            if not any(s in params[i].name for s in self.exclude):\n                delta = self.compute_perturbation(params[i], params_gradients[i])\n                self.trainable_variables[i].assign_sub(delta)\n\n        #if nan is detected, skip update\n        # nan_detected = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients])\n        # _ = tf.cond(nan_detected, lambda:tf.constant(False),lambda:self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)))\n\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        self.compiled_metrics.update_state(y, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n    def train_step(self, data):\n        return tf.cond(self._train_counter < self.start_step, lambda:super(AWP,self).train_step(data), lambda:self.train_step_awp(data))","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:16.501617Z","iopub.execute_input":"2024-09-26T10:02:16.502403Z","iopub.status.idle":"2024-09-26T10:02:16.514993Z","shell.execute_reply.started":"2024-09-26T10:02:16.502359Z","shell.execute_reply":"2024-09-26T10:02:16.514079Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def get_model(max_len=128, target_len=64, dim=192, dtype='float32'):\n    ################# ENCODER #################\n    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp1)\n    ksize = 17\n    drop_rate = 0.2\n    x = tf.keras.layers.Dense(dim,use_bias=False,name='stem_conv')(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=0,strides=2)(x) #drop_rate=0 since we don't want to drop the whole output here\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n\n    encoder = tf.keras.Model(inp1,x,name='encoder')\n\n    ################# CTC DECDODER #################\n    inp3 = tf.keras.Input((x.shape[1],dim),name='ctc_decoder_inp2',dtype=dtype)\n    x = inp3\n    x = tf.keras.layers.RNN(tf.keras.layers.GRUCell(dim), return_sequences=True)(x)\n    x = tf.keras.layers.Dense(dim*2)(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='ctc_classifier')(x) #include sos, eos token\n    ctc_decoder = tf.keras.Model(inp3,x,name='ctc_decoder')\n\n    ################# ATT DECODER #################\n    inp2 = tf.keras.Input((None,),name='att_decoder_inp1',dtype='int32')\n    inp3 = tf.keras.Input((x.shape[1],dim),name='att_decoder_inp2',dtype=dtype)\n\n    x = inp3\n    y = tf.keras.layers.Masking(mask_value=0,input_shape=(None,),name='att_decoder_input_masking')(inp2)\n    y = tf.keras.layers.Embedding(NUM_CLASSES,dim,mask_zero=True,name='att_decoder_token_emb')(y) #include sos token\n    y = PosEmbedding(dim,max_len=target_len,name='att_decoder_pos_emb')(y)\n    y = TransformerDecoderBlock(dim,expand=2,num_heads=4,attn_dropout=0.2,name='att_decoder_block1')(y,x,x)\n    y = tf.keras.layers.Dropout(0.5)(y)\n    y = tf.keras.layers.Dense(NUM_CLASSES,name='att_decoder_classifier')(y)\n\n    decoder = tf.keras.Model([inp2,inp3],y,name='att_decoder')\n\n    ################### MODEL #####################\n    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n    inp2 = tf.keras.Input((None,),dtype='int32')\n\n    x = inp1\n    enc_out = encoder(x)\n    y = inp2\n    dec_out = decoder([y, enc_out])\n    ctc_out = ctc_decoder(enc_out)\n    model = tf.keras.Model([inp1,inp2], [dec_out,ctc_out])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:19.346190Z","iopub.execute_input":"2024-09-26T10:02:19.346544Z","iopub.status.idle":"2024-09-26T10:02:19.364071Z","shell.execute_reply.started":"2024-09-26T10:02:19.346516Z","shell.execute_reply":"2024-09-26T10:02:19.363170Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class CSVLoggerV2(tf.keras.callbacks.CSVLogger):\n    def __init__(self, filename, separator=\",\", resume=0):\n        self.resume = resume\n        super().__init__(filename=filename, separator=separator, append=bool(resume))\n\n    def on_epoch_end(self, epoch, logs=None):\n        super(CSVLoggerV2,self).on_epoch_end(epoch=epoch+self.resume+1, logs=logs)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:22.348521Z","iopub.execute_input":"2024-09-26T10:02:22.348852Z","iopub.status.idle":"2024-09-26T10:02:22.353970Z","shell.execute_reply.started":"2024-09-26T10:02:22.348828Z","shell.execute_reply":"2024-09-26T10:02:22.353100Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def train_fold(CFG, fold, train_files, valid_files=None, strategy=STRATEGY, summary=True):\n    seed_everything(CFG.seed)\n    tf.keras.backend.clear_session()\n    gc.collect()\n    # tf.config.optimizer.set_jit(True)\n\n    policy = mixed_precision.Policy(CFG.policy)\n    mixed_precision.set_global_policy(policy)\n\n    if CFG.resume == 'auto':\n        if os.path.isfile(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv'):\n            resume = pd.read_csv(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')['epoch'].values[-1]\n            resume = 0 if resume == CFG.epoch else resume #restart if training is already fininshed\n        else:\n            resume = 0\n    else:\n        resume = CFG.resume\n\n    if fold != 'all':\n        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.train_batch_size, max_len=CFG.max_len, drop_remainder=True, train=True, augment=True, repeat=True, shuffle=4096)\n        valid_ds = get_tfrec_dataset(valid_files, batch_size=CFG.valid_batch_size, max_len=CFG.max_len, drop_remainder=True, repeat=False, shuffle=False)\n    else:\n        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.train_batch_size, max_len=CFG.max_len, drop_remainder=True, train=True, augment=True, repeat=True, shuffle=4096)\n        valid_ds = None\n        valid_files = []\n\n    num_train = count_data_items(train_files)\n    num_valid = count_data_items(valid_files)\n    steps_per_epoch = num_train//CFG.train_batch_size\n    with strategy.scope():\n        model = get_model(max_len=CFG.max_len, dim=CFG.dim, dtype='bfloat16') #dtype should be matched with CFG.policy\n\n        schedule = OneCycleLR(CFG.lr, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min, decay_type=CFG.decay_type, warmup_type=CFG.warmup_type)\n        decay_schedule = OneCycleLR(CFG.lr*CFG.weight_decay, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min*CFG.weight_decay, decay_type=CFG.decay_type, warmup_type=CFG.warmup_type)\n\n        awp_start_epoch = max(CFG.awp_start_epoch - resume, 0)\n        awp_step = int(awp_start_epoch * steps_per_epoch)\n        if CFG.fgm:\n            model = FGM(model.input, model.output, lr=CFG.awp_lr, eps=0., start_step=awp_step)\n        elif CFG.awp:\n            model = AWP(model.input, model.output, lr=CFG.awp_lr, eps=0., start_step=awp_step, exclude=['bias','gamma','beta','rnn'])\n\n        # opt = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4)\n        # opt = tfa.optimizers.Lookahead(opt,sync_period=5)\n        opt = tfa.optimizers.AdamW(learning_rate=schedule, weight_decay=decay_schedule)\n\n        model.compile(\n            optimizer=opt,\n            loss=[MaskedSCCE(label_smoothing=0.25), CTCLoss()],\n            loss_weights=[0.75,0.25],\n            metrics=[\n                [\n                Accuracy(),\n                ],\n                [],\n            ],\n        )\n\n    if summary:\n        print()\n        model.summary()\n        print()\n        print(train_ds, valid_ds)\n        print()\n        schedule.plot()\n        print()\n        init=False\n    print(f'---------fold{fold}---------')\n    print(f'train:{num_train} valid:{num_valid}')\n    print()\n\n    if resume:\n        print(f'resume from epoch{resume}')\n        if CFG.resume_ckpt:\n            print(f'load weights from {CFG.resume_ckpt}')\n            model.load_weights(CFG.resume_ckpt)\n        else:\n            print(f'load weights from {CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n            model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n        # if train_ds is not None:\n        #     model.evaluate(train_ds.take(steps_per_epoch))\n        # if valid_ds is not None:\n        #     model.evaluate(valid_ds)\n\n    logger = CSVLoggerV2(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv', resume=resume)\n\n    mode = 'min'\n    if fold != 'all':\n        monitor = 'val_loss'\n    else:\n        monitor = 'loss'\n    if resume:\n        prev_best = pd.read_csv(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')[monitor].agg(mode)\n    else:\n        prev_best = None\n    sv_loss = tf.keras.callbacks.ModelCheckpoint(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5', monitor=monitor, verbose=0, save_best_only=True,\n                  save_weights_only=True, mode='min', save_freq='epoch', initial_value_threshold=prev_best)\n\n    snap = Snapshot(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', snapshot_epochs=[])\n    # swa = SWA(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.swa_epochs, strategy=strategy, train_ds=train_ds, valid_ds=valid_ds)\n\n    callbacks = []\n    if CFG.save_output:\n        callbacks.append(logger)\n        callbacks.append(snap)\n        # callbacks.append(swa)\n        callbacks.append(sv_loss)\n\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epoch-resume,\n        steps_per_epoch=steps_per_epoch,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        verbose=CFG.verbose,\n    )\n\n    if fold != 'all':\n        ds = get_tfrec_dataset(valid_files, batch_size=CFG.valid_batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n        labels = extract_labels(ds)\n        model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n        print('ATTENTION EVAL')\n        evaluate(GreedyDecoder(model),ds,labels)\n        print()\n        print('CTC EVAL')\n        evaluate(KerasCTCDecoder(model, greedy=True),ds,labels)\n\n    return model, history\n\ndef train_folds(CFG, folds, strategy=STRATEGY, summary=True):\n    for fold in folds:\n        if fold != 'all':\n            all_files = TRAIN_FILENAMES\n            train_files = [x for x in all_files if f'fold{fold}' not in x]\n            valid_files = [x for x in all_files if f'fold{fold}' in x]\n        else:\n            train_files = TRAIN_FILENAMES\n            valid_files = None\n\n        train_fold(CFG, fold, train_files, valid_files, strategy=strategy, summary=summary)\n    return","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:24.619153Z","iopub.execute_input":"2024-09-26T10:02:24.619547Z","iopub.status.idle":"2024-09-26T10:02:24.645184Z","shell.execute_reply.started":"2024-09-26T10:02:24.619516Z","shell.execute_reply":"2024-09-26T10:02:24.644240Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    n_splits = 5\n    save_output = True\n    output_dir = '.'\n\n    seed = 42\n    verbose = 'auto' #0) silent 1) progress bar 2) one line per epoch\n\n    dim = 192\n    max_len = 768\n\n    policy = 'mixed_bfloat16' #'float32') fp32, 'mixed_float16') GPU+fp16, 'mixed_bfloat16') TPU+fp16\n    replicas = N_REPLICAS\n    lr = 5e-4 * replicas\n    weight_decay = 0.01\n    lr_min = 1e-6\n    epoch = 60 #400\n    warmup = 0.1\n    warmup_type = 'linear'\n    decay_type = 'cosine'\n    train_batch_size = 16 * replicas\n    valid_batch_size = 64 * replicas\n\n    fgm = False\n    awp = False #True\n    awp_lr = 0.2\n    awp_start_epoch = 0.1 * epoch\n\n    resume = 0\n    resume_ckpt = ''\n    comment =  f'aslfr-fp16-192d-17l-ctcattjoint-seed{seed}'","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:02:31.785055Z","iopub.execute_input":"2024-09-26T10:02:31.786082Z","iopub.status.idle":"2024-09-26T10:02:31.792219Z","shell.execute_reply.started":"2024-09-26T10:02:31.786044Z","shell.execute_reply":"2024-09-26T10:02:31.791335Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def decode_tfrec(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'coordinates': tf.io.FixedLenFeature([], tf.string),\n        'phrase_encoded': tf.io.VarLenFeature(dtype=tf.int64),\n        'phrase': tf.io.FixedLenFeature([], tf.string),\n    })\n    out = {}\n    out['coordinates']  = tf.transpose(tf.reshape(tf.io.decode_raw(features['coordinates'], tf.float32), (-1,3,ROWS_PER_FRAME)), (0,2,1))\n    out['phrase'] = features['phrase']\n    return out\n\ndef get_tfrec_dataset(tfrecords, batch_size=64, max_len=128, target_len=64, teacher_forcing=True, drop_remainder=False, train=False, augment=False, shuffle=False, repeat=False):\n    # Initialize dataset with TFRecords\n    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n    ds = ds.map(lambda x: preprocess(x, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n\n    if train:\n        ds = ds.filter(is_empty)\n\n    if teacher_forcing:\n        ds = ds.map(lambda x,y:((x,y[:-1]),(y[1:],y[1:-1])), tf.data.AUTOTUNE)\n        if augment:\n            ds = ds.map(lambda x,y:((x[0],augment_phrase(x[1])),y), tf.data.AUTOTUNE)\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(shuffle)\n        options = tf.data.Options()\n        options.experimental_deterministic = (False)\n        ds = ds.with_options(options)\n\n    if batch_size:\n        if teacher_forcing:\n            ds = ds.padded_batch(batch_size, padding_values=((PAD,0),(0,0)), padded_shapes=(([max_len,CHANNELS],[target_len,]),([target_len,],[target_len,])), drop_remainder=drop_remainder)\n        else:\n            ds = ds.padded_batch(batch_size, padding_values=(PAD,0), padded_shapes=([max_len,CHANNELS],[target_len,]), drop_remainder=drop_remainder)\n\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n\n    return ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Why does this work for asl tfrecords\nTRAIN_FILENAMES = tf.io.gfile.glob('/kaggle/input/aslfr-5fold/*.tfrecords')\n\nds = get_tfrec_dataset(TRAIN_FILENAMES, train=True, augment=True, batch_size=4, shuffle=4)\nprint(ds)\nfor x in ds:\n    temp_train = x\n    break","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:13:28.632567Z","iopub.execute_input":"2024-09-26T10:13:28.632987Z","iopub.status.idle":"2024-09-26T10:13:30.582266Z","shell.execute_reply.started":"2024-09-26T10:13:28.632958Z","shell.execute_reply":"2024-09-26T10:13:30.580773Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(None, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(None, 64), dtype=tf.int32, name=None), TensorSpec(shape=(None, 64), dtype=tf.int32, name=None)))>\n","output_type":"stream"}]},{"cell_type":"code","source":"# This doesn't work for auslan tfrecords\nTRAIN_FILENAMES = tf.io.gfile.glob('/kaggle/input/ausslfr-5fold/*.tfrecords')\n\nds = get_tfrec_dataset(TRAIN_FILENAMES, train=True, augment=True, batch_size=1, shuffle=1)\nprint(ds)\nfor x in ds:\n    temp_train = x\n    break","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:33:07.891260Z","iopub.execute_input":"2024-09-26T10:33:07.892243Z","iopub.status.idle":"2024-09-26T10:33:09.659160Z","shell.execute_reply.started":"2024-09-26T10:33:07.892207Z","shell.execute_reply":"2024-09-26T10:33:09.657819Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(None, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(None, 64), dtype=tf.int32, name=None), TensorSpec(shape=(None, 64), dtype=tf.int32, name=None)))>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m get_tfrec_dataset(TRAIN_FILENAMES, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ds)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ds:\n\u001b[1;32m      7\u001b[0m     temp_train \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    796\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 780\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3018\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} input image must be of non-zero size\n\t [[{{function_node cond_1_cond_true_245562}}{{node cond_1/cond/resize/ResizeBilinear}}]] [Op:IteratorGetNext]"],"ename":"InvalidArgumentError","evalue":"{{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} input image must be of non-zero size\n\t [[{{function_node cond_1_cond_true_245562}}{{node cond_1/cond/resize/ResizeBilinear}}]] [Op:IteratorGetNext]","output_type":"error"}]},{"cell_type":"code","source":"model = get_model()\n#y = model(temp_train[0], training=True)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:25:22.532605Z","iopub.execute_input":"2024-09-26T10:25:22.533000Z","iopub.status.idle":"2024-09-26T10:25:27.385697Z","shell.execute_reply.started":"2024-09-26T10:25:22.532970Z","shell.execute_reply":"2024-09-26T10:25:27.384809Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 128, 1629)]  0           []                               \n                                                                                                  \n input_3 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n encoder (Functional)           (None, 64, 192)      5565377     ['input_2[0][0]']                \n                                                                                                  \n att_decoder (Functional)       (None, None, 62)     480830      ['input_3[0][0]',                \n                                                                  'encoder[0][0]']                \n                                                                                                  \n ctc_decoder (Functional)       (None, 64, 62)       320318      ['encoder[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 6,366,525\nTrainable params: 6,336,957\nNon-trainable params: 29,568\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#check supports_masking\nfor x in model.layers:\n    if not x.supports_masking:\n        print(x.supports_masking, x.name)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:25:34.951432Z","iopub.execute_input":"2024-09-26T10:25:34.951856Z","iopub.status.idle":"2024-09-26T10:25:34.956690Z","shell.execute_reply.started":"2024-09-26T10:25:34.951823Z","shell.execute_reply":"2024-09-26T10:25:34.955839Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"CFG.resume = 'auto'\ntrain_folds(CFG, [0])","metadata":{"execution":{"iopub.status.busy":"2024-09-26T10:25:37.855543Z","iopub.execute_input":"2024-09-26T10:25:37.855939Z","iopub.status.idle":"2024-09-26T10:28:59.209174Z","shell.execute_reply.started":"2024-09-26T10:25:37.855909Z","shell.execute_reply":"2024-09-26T10:28:59.207731Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 768, 1629)]  0           []                               \n                                                                                                  \n input_3 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n encoder (Functional)           (None, 384, 192)     5565377     ['input_2[0][0]']                \n                                                                                                  \n att_decoder (Functional)       (None, None, 62)     480830      ['input_3[0][0]',                \n                                                                  'encoder[0][0]']                \n                                                                                                  \n ctc_decoder (Functional)       (None, 384, 62)      320318      ['encoder[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 6,366,525\nTrainable params: 6,336,957\nNon-trainable params: 29,568\n__________________________________________________________________________________________________\n\n<_PrefetchDataset element_spec=((TensorSpec(shape=(128, 768, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(128, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(128, 64), dtype=tf.int32, name=None), TensorSpec(shape=(128, 64), dtype=tf.int32, name=None)))> <_PrefetchDataset element_spec=((TensorSpec(shape=(512, 768, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(512, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(512, 64), dtype=tf.int32, name=None), TensorSpec(shape=(512, 64), dtype=tf.int32, name=None)))>\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\n---------fold0---------\ntrain:2256 valid:846\n\nEpoch 1/60\n","output_type":"stream"},{"name":"stderr","text":"2024-09-26 10:27:37.454913: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AdamW/AdamW/AssignAddVariableOp.\n2024-09-26 10:27:41.147493: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AdamW/AdamW/AssignAddVariableOp.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m CFG\u001b[38;5;241m.\u001b[39mresume \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[53], line 140\u001b[0m, in \u001b[0;36mtrain_folds\u001b[0;34m(CFG, folds, strategy, summary)\u001b[0m\n\u001b[1;32m    137\u001b[0m         train_files \u001b[38;5;241m=\u001b[39m TRAIN_FILENAMES\n\u001b[1;32m    138\u001b[0m         valid_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n","Cell \u001b[0;32mIn[53], line 109\u001b[0m, in \u001b[0;36mtrain_fold\u001b[0;34m(CFG, fold, train_files, valid_files, strategy, summary)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# callbacks.append(swa)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(sv_loss)\n\u001b[0;32m--> 109\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fold \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    119\u001b[0m     ds \u001b[38;5;241m=\u001b[39m get_tfrec_dataset(valid_files, batch_size\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mvalid_batch_size, max_len\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mmax_len, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n7 root error(s) found.\n  (0) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_23]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_28]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_7]]\n  (1) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_23]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_28]]\n  (2) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_23]]\n  (3) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n  (4) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n  (5) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n  (6) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n0 successful operations.\n2 derived errors ignored. [Op:__inference_train_function_245473]"],"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\n7 root error(s) found.\n  (0) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_23]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_28]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_7]]\n  (1) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_23]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_28]]\n  (2) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_23]]\n  (3) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n\t [[TPUReplicate/_compile/_12126411305052820135/_2/_60]]\n  (4) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n\t [[tpu_compile_succeeded_assert/_405172905470871505/_3/_47]]\n  (5) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n\t [[cluster_train_function/_execute_7_0/_95]]\n  (6) INVALID_ARGUMENT:  input image must be of non-zero size\n\t [[{{node cond_1/cond/cond/resize/ResizeNearestNeighbor}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[IteratorGetNext]]\n0 successful operations.\n2 derived errors ignored. [Op:__inference_train_function_245473]","output_type":"error"}]}]}