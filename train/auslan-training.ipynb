{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":6147567,"sourceType":"datasetVersion","datasetId":3376501},{"sourceId":6156851,"sourceType":"datasetVersion","datasetId":3531756},{"sourceId":9485369,"sourceType":"datasetVersion","datasetId":5769927},{"sourceId":9538837,"sourceType":"datasetVersion","datasetId":5770267},{"sourceId":9539459,"sourceType":"datasetVersion","datasetId":5768310}],"dockerImageVersionId":30529,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Move CTC_TPU for local use\nfrom shutil import copyfile\ncopyfile(src = \"/kaggle/input/ctc-tpu/CTC_TPU.py\", dst = \"/kaggle/working//CTC_TPU.py\")\n\nfrom CTC_TPU import classic_ctc_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:12:29.634629Z","iopub.execute_input":"2024-10-06T02:12:29.634855Z","iopub.status.idle":"2024-10-06T02:13:12.811559Z","shell.execute_reply.started":"2024-10-06T02:12:29.634832Z","shell.execute_reply":"2024-10-06T02:13:12.810475Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"D1006 02:13:01.181130524      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD1006 02:13:01.181178107      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD1006 02:13:01.181185109      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD1006 02:13:01.181188635      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD1006 02:13:01.181191346      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD1006 02:13:01.181194035      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD1006 02:13:01.181196385      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD1006 02:13:01.181199257      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD1006 02:13:01.181201418      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD1006 02:13:01.181203593      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD1006 02:13:01.181205864      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD1006 02:13:01.181208269      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD1006 02:13:01.181210734      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD1006 02:13:01.181213128      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI1006 02:13:01.181432886      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\nD1006 02:13:01.181447325      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD1006 02:13:01.181492692      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD1006 02:13:01.190539608      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1006 02:13:01.190555241      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1006 02:13:01.190559130      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1006 02:13:01.190562582      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1006 02:13:01.190566068      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1006 02:13:01.190569120      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD1006 02:13:01.190577533      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1006 02:13:01.190610887      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1006 02:13:01.190655170      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1006 02:13:01.190673748      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1006 02:13:01.190677469      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1006 02:13:01.190681667      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1006 02:13:01.190689870      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD1006 02:13:01.190693537      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1006 02:13:01.190697123      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1006 02:13:01.190701796      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI1006 02:13:01.194375566      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI1006 02:13:01.208477752     239 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1006 02:13:01.214250314     239 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-10-06T02:13:01.214234549+00:00\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q tensorflow-addons\n!pip install -q git+https://github.com/hoyso48/tf-utils@main\n!pip install -q Levenshtein\n!pip install -q keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:12.813362Z","iopub.execute_input":"2024-10-06T02:13:12.813881Z","iopub.status.idle":"2024-10-06T02:13:36.619585Z","shell.execute_reply.started":"2024-10-06T02:13:12.813842Z","shell.execute_reply":"2024-10-06T02:13:36.618215Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"!pip freeze > requirements.txt\n!cat requirements.txt\n\nimport sys\nprint(f\"Python version: {sys.version}\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:36.621052Z","iopub.execute_input":"2024-10-06T02:13:36.621384Z","iopub.status.idle":"2024-10-06T02:13:36.630770Z","shell.execute_reply.started":"2024-10-06T02:13:36.621352Z","shell.execute_reply":"2024-10-06T02:13:36.630008Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'!pip freeze > requirements.txt\\n!cat requirements.txt\\n\\nimport sys\\nprint(f\"Python version: {sys.version}\")'"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras_nlp\nimport tensorflow.keras.mixed_precision as mixed_precision\n\nfrom tf_utils.schedules import OneCycleLR, ListedLR\nfrom tf_utils.callbacks import Snapshot, SWA\nfrom tf_utils.learners import FGM, AWP\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom tqdm.autonotebook import tqdm\nimport sklearn\n\nimport os\nimport time\nimport pickle\nimport math\nimport random\nimport sys\nimport cv2\nimport gc\nimport glob\nimport datetime\n\nfrom Levenshtein import distance\n\nprint(f'Tensorflow Version: {tf.__version__}')\nprint(f'Python Version: {sys.version}')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:36.631823Z","iopub.execute_input":"2024-10-06T02:13:36.632093Z","iopub.status.idle":"2024-10-06T02:13:40.074394Z","shell.execute_reply.started":"2024-10-06T02:13:36.632070Z","shell.execute_reply":"2024-10-06T02:13:40.073438Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n/tmp/ipykernel_15/864907894.py:15: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow Version: 2.12.0\nPython Version: 3.8.17 (default, Jul  4 2023, 06:27:59) \n[GCC 12.2.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Seed all random number generators\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# Accelerator settings\ndef get_strategy(device='TPU'):\n    if \"TPU\" in device:\n        tpu = 'local' if device=='TPU-VM' else None\n        print(\"connecting to TPU...\")\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        IS_TPU = True\n\n    if device == \"GPU\"  or device==\"CPU\":\n        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n        if ngpu>1:\n            print(\"Using multi GPU\")\n            strategy = tf.distribute.MirroredStrategy()\n        elif ngpu==1:\n            print(\"Using single GPU\")\n            strategy = tf.distribute.get_strategy()\n        else:\n            print(\"Using CPU\")\n            strategy = tf.distribute.get_strategy()\n        IS_TPU = False\n\n    if device == \"GPU\":\n        print(\"Num GPUs Available: \", ngpu)\n\n    AUTO     = tf.data.experimental.AUTOTUNE\n    REPLICAS = strategy.num_replicas_in_sync\n    print(f'REPLICAS: {REPLICAS}')\n\n    return strategy, REPLICAS, IS_TPU\n\nSTRATEGY, N_REPLICAS, IS_TPU = get_strategy('TPU-VM')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:40.076936Z","iopub.execute_input":"2024-10-06T02:13:40.077643Z","iopub.status.idle":"2024-10-06T02:13:44.298957Z","shell.execute_reply.started":"2024-10-06T02:13:40.077612Z","shell.execute_reply":"2024-10-06T02:13:44.298131Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"connecting to TPU...\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialise tfrecord files\nTRAIN_FILENAMES = tf.io.gfile.glob('/kaggle/input/ausslfr-5fold/*.tfrecords')\n\nprint(len(TRAIN_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.299934Z","iopub.execute_input":"2024-10-06T02:13:44.300185Z","iopub.status.idle":"2024-10-06T02:13:44.314460Z","shell.execute_reply.started":"2024-10-06T02:13:44.300143Z","shell.execute_reply":"2024-10-06T02:13:44.313721Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"code","source":"# Label dictionary\nimport json\nfrom IPython.display import display\n\nwith open('/kaggle/input/australian-fingerspelling/character_to_prediction_index.json') as json_file:\n    CHAR_TO_NUM = json.load(json_file)\nNUM_TO_CHAR = dict([(y+1,x) for x,y in CHAR_TO_NUM.items()] )\nNUM_TO_CHAR[60] = 'S'\nNUM_TO_CHAR[61] = 'E'\nNUM_TO_CHAR[0] = 'P'\n\nTABLE = tf.lookup.StaticHashTable(\n    initializer=tf.lookup.KeyValueTensorInitializer(\n        keys=list(NUM_TO_CHAR.values()),\n        values=list(NUM_TO_CHAR.keys()),\n    ),\n    default_value=tf.constant(-1),\n    name=\"class_weight\"\n)\n\n# Train data frame\ntrain_df = pd.read_csv('/kaggle/input/australian-fingerspelling/train.csv')\ndisplay(train_df.head())\ndisplay(train_df.info())","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.315475Z","iopub.execute_input":"2024-10-06T02:13:44.315760Z","iopub.status.idle":"2024-10-06T02:13:44.367691Z","shell.execute_reply.started":"2024-10-06T02:13:44.315739Z","shell.execute_reply":"2024-10-06T02:13:44.366901Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"                    path  file_id  sequence_id  participant_id  \\\n0  landmarks/101.parquet      101        10101               1   \n1  landmarks/101.parquet      101        10104               1   \n2  landmarks/101.parquet      101        10105               1   \n3  landmarks/101.parquet      101        10106               1   \n4  landmarks/101.parquet      101        10107               1   \n\n                       phrase  \n0  abcdefghijklmnopqrstuvwxyz  \n1                  antecedent  \n2                 antifascist  \n3                   cooperate  \n4                    describe  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>file_id</th>\n      <th>sequence_id</th>\n      <th>participant_id</th>\n      <th>phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10101</td>\n      <td>1</td>\n      <td>abcdefghijklmnopqrstuvwxyz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10104</td>\n      <td>1</td>\n      <td>antecedent</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10105</td>\n      <td>1</td>\n      <td>antifascist</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10106</td>\n      <td>1</td>\n      <td>cooperate</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>landmarks/101.parquet</td>\n      <td>101</td>\n      <td>10107</td>\n      <td>1</td>\n      <td>describe</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3933 entries, 0 to 3932\nData columns (total 5 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   path            3933 non-null   object\n 1   file_id         3933 non-null   int64 \n 2   sequence_id     3933 non-null   int64 \n 3   participant_id  3933 non-null   int64 \n 4   phrase          3933 non-null   object\ndtypes: int64(3), object(2)\nmemory usage: 153.8+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"code","source":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename.split('/')[-1]).group(1)) for filename in filenames]\n    return np.sum(n)\nprint(count_data_items(TRAIN_FILENAMES), len(train_df))\nassert count_data_items(TRAIN_FILENAMES) == len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.368618Z","iopub.execute_input":"2024-10-06T02:13:44.368886Z","iopub.status.idle":"2024-10-06T02:13:44.374522Z","shell.execute_reply.started":"2024-10-06T02:13:44.368863Z","shell.execute_reply":"2024-10-06T02:13:44.373797Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"3933 3933\n","output_type":"stream"}]},{"cell_type":"code","source":"#for the lip_lr function. LEFT[i] is matching with RIGHT[i](i.e LEFT[i](x) == -RIGHT[i](x)).\n#computed from https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model.obj\n\nLEFT = [\n         248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n         265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n         282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n         299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n         316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332,\n         333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n         367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n         384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n         401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n         418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n         435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,  #LFACE\n         468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, #LHAND\n         493, 494, 495, 497, 499, 501, 503, 505, 507, 509, 511, 513, #LPOSE\n         515, 517, 519, 521, #LLEG\n         ]\n\nRIGHT = [\n         3, 7, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n         39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n         60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n         81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102,\n         103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n         121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n         139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158,\n         159, 160, 161, 162, 163, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179,\n         180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 201,\n         202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, #RFACE\n        522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, #RHAND\n        490, 491, 492, 496, 498, 500, 502, 504, 506, 508, 510, 512, #RPOSE\n        514, 516, 518, 520, #RLEG\n        ]\n\nCENTRE = [\n          0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 94, 151, 152, 164, 168, 175, 195, 197, 199, 200, #FACE\n          489, #POSE\n          ]\n\nprint(len(LEFT+RIGHT+CENTRE))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.375767Z","iopub.execute_input":"2024-10-06T02:13:44.376038Z","iopub.status.idle":"2024-10-06T02:13:44.397683Z","shell.execute_reply.started":"2024-10-06T02:13:44.376014Z","shell.execute_reply":"2024-10-06T02:13:44.397011Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"543\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialise variables for training\nROWS_PER_FRAME = 543\nMAX_LEN = 384\nCROP_LEN = MAX_LEN\nNUM_CLASSES  = len(NUM_TO_CHAR.values()) #62\nPAD = -100.\n\nLHAND = np.arange(468, 489).tolist()\nRHAND = np.arange(522, 543).tolist()\nPOINT_LANDMARKS = list(range(543))\n\nNUM_NODES = len(POINT_LANDMARKS)\nCHANNELS = 3*NUM_NODES\n\nprint(NUM_NODES)\nprint(CHANNELS)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.398671Z","iopub.execute_input":"2024-10-06T02:13:44.398923Z","iopub.status.idle":"2024-10-06T02:13:44.410997Z","shell.execute_reply.started":"2024-10-06T02:13:44.398901Z","shell.execute_reply":"2024-10-06T02:13:44.410319Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"543\n1629\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Function Definitions","metadata":{}},{"cell_type":"code","source":"def preprocess_phrase(phrase, table=TABLE):\n    phrase = tf.strings.join(['S', phrase, 'E']) #'S'+ phrase + 'E'\n    phrase = tf.strings.bytes_split(phrase)\n    phrase = table.lookup(phrase)\n    return phrase\n\ndef interp1d_(x, target_len, method='random'):\n    length = tf.shape(x)[1]\n    target_len = tf.maximum(1,target_len)\n    if method == 'random':\n        if tf.random.uniform(()) < 0.33:\n            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n        else:\n            if tf.random.uniform(()) < 0.5:\n                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n            else:\n                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n    else:\n        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n    return x\n\ndef tf_nan_mean(x, axis=0, keepdims=False):\n    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n\ndef tf_nan_std(x, center=None, axis=0, keepdims=False):\n    if center is None:\n        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n    d = x - center\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n\ndef is_left_handed(x):\n    lhand = tf.gather(x, LHAND, axis=1)\n    rhand = tf.gather(x, RHAND, axis=1)\n    lhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(lhand), tf.int32))\n    rhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(rhand), tf.int32))\n    return lhand_nans < rhand_nans\n\ndef flip_lr(x, left=LEFT, right=RIGHT):\n    x,y,z = tf.unstack(x, axis=-1)\n    x = 1-x\n    new_x = tf.stack([x,y,z], -1)\n    new_x = tf.transpose(new_x, [1,0,2])\n    l_x = tf.gather(new_x, left, axis=0)\n    r_x = tf.gather(new_x, right, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(left)[...,None], r_x)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(right)[...,None], l_x)\n    new_x = tf.transpose(new_x, [1,0,2])\n    return new_x\n\nclass Preprocess(tf.keras.layers.Layer):\n    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.point_landmarks = point_landmarks\n\n    def call(self, inputs):\n        # if tf.rank(inputs) == 3:\n        #     x = inputs[None,...]\n        # else:\n        #     x = inputs\n        x = inputs\n        x = filter_nans_tf(x)\n        x = tf.cond(is_left_handed(x), lambda:flip_lr(x), lambda:x)\n        x = x[None,...]\n\n        if self.max_len is not None:\n            x = x[:,:self.max_len]\n        length = tf.shape(x)[1]\n\n        mean = tf_nan_mean(tf.gather(x, self.point_landmarks, axis=2), axis=[1,2], keepdims=True)\n        mean = tf.where(tf.math.is_nan(mean), tf.constant([0.5,0.5,0.],x.dtype), mean)\n        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n\n        x = (x - mean)/std\n\n        x = tf.concat([\n            tf.reshape(x, (-1,length,3*len(self.point_landmarks))),\n            # tf.reshape(dx, (-1,length,3*len(self.point_landmarks))),\n        ], axis = -1)\n\n        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.412079Z","iopub.execute_input":"2024-10-06T02:13:44.412375Z","iopub.status.idle":"2024-10-06T02:13:44.430842Z","shell.execute_reply.started":"2024-10-06T02:13:44.412351Z","shell.execute_reply":"2024-10-06T02:13:44.430183Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def decode_tfrec(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'coordinates': tf.io.FixedLenFeature([], tf.string),\n        'phrase_encoded': tf.io.VarLenFeature(dtype=tf.int64),\n        'phrase': tf.io.FixedLenFeature([], tf.string),\n    })\n    out = {}\n    out['coordinates']  = tf.transpose(tf.reshape(tf.io.decode_raw(features['coordinates'], tf.float32), (-1,3,ROWS_PER_FRAME)), (0,2,1))\n    out['phrase'] = features['phrase']\n    return out\n\ndef filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n    x = tf.boolean_mask(x, mask, axis=0)\n    return x\n\ndef preprocess(x, augment=False, max_len=MAX_LEN):\n    coord = x['coordinates']\n    if augment:\n        coord = augment_fn(coord, max_len=max_len)\n    coord = tf.ensure_shape(coord, (None,ROWS_PER_FRAME,3))\n\n    inp = tf.cast(Preprocess(max_len=max_len)(coord)[0],tf.float32)\n    tar = preprocess_phrase(x['phrase'])\n\n    return inp, tar\n\ndef augment_phrase(phrase):\n    phrase = keras_nlp.layers.MaskedLMMaskGenerator(NUM_CLASSES-2,\n                                              mask_selection_rate=0.2,\n                                              mask_token_id=0,\n                                              mask_token_rate=0,\n                                              random_token_rate=1,\n                                              unselectable_token_ids=[0,60,61])(phrase)['token_ids']\n    return phrase\n\ndef is_empty(*args):\n    return tf.shape(args[0])[0] > 1\n\ndef resample(x, rate=(0.8,1.2)):\n    rate = tf.random.uniform((), rate[0], rate[1])\n    length = tf.shape(x)[0]\n    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n    new_x = interp1d_(x, new_size)\n    return new_x\n\ndef spatial_random_affine(xyz,\n    scale  = (0.8,1.2),\n    shear = (-0.15,0.15),\n    shift  = (-0.1,0.1),\n    degree = (-30,30),\n):\n    center = tf.constant([0.5,0.5])\n    if scale is not None:\n        scale = tf.random.uniform((),*scale)\n        xyz = scale*xyz\n\n    if shear is not None:\n        xy = xyz[...,:2]\n        z = xyz[...,2:]\n        shear_x = shear_y = tf.random.uniform((),*shear)\n        if tf.random.uniform(()) < 0.5:\n            shear_x = 0.\n        else:\n            shear_y = 0.\n        shear_mat = tf.identity([\n            [1.,shear_x],\n            [shear_y,1.]\n        ])\n        xy = xy @ shear_mat\n        center = center + [shear_y, shear_x]\n        xyz = tf.concat([xy,z], axis=-1)\n\n    if degree is not None:\n        xy = xyz[...,:2]\n        z = xyz[...,2:]\n        xy -= center\n        degree = tf.random.uniform((),*degree)\n        radian = degree/180*np.pi\n        c = tf.math.cos(radian)\n        s = tf.math.sin(radian)\n        rotate_mat = tf.identity([\n            [c,s],\n            [-s, c],\n        ])\n        xy = xy @ rotate_mat\n        xy = xy + center\n        xyz = tf.concat([xy,z], axis=-1)\n\n    if shift is not None:\n        shift = tf.random.uniform((),*shift)\n        xyz = xyz + shift\n\n    return xyz\n\ndef temporal_crop(x, length=MAX_LEN):\n    l = tf.shape(x)[0]\n    offset = tf.random.uniform((), 0, tf.clip_by_value(l-length,1,length), dtype=tf.int32)\n    x = x[offset:offset+length]\n    return x\n\ndef temporal_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n    l = tf.shape(x)[0]\n    mask_size = tf.random.uniform((), *size)\n    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l-mask_size,1,l), dtype=tf.int32)\n    x = tf.tensor_scatter_nd_update(x,tf.range(mask_offset, mask_offset+mask_size)[...,None],tf.fill([mask_size,543,3],mask_value))\n    return x\n\ndef spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n    mask_offset_y = tf.random.uniform(())\n    mask_offset_x = tf.random.uniform(())\n    mask_size = tf.random.uniform((), *size)\n    mask_x = (mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size)\n    mask_y = (mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size)\n    mask = mask_x & mask_y\n    x = tf.where(mask[...,None], mask_value, x)\n    return x\n\ndef augment_fn(x, always=False, max_len=None):\n    if tf.random.uniform(())<0.8 or always:\n        x = resample(x, (0.5,1.5))\n    # if tf.random.uniform(())<0.5 or always:\n    #     x = flip_lr(x)\n    # if max_len is not None:\n    #     x = temporal_crop(x, max_len)\n    if tf.random.uniform(())<0.75 or always:\n        x = spatial_random_affine(x)\n    # if tf.random.uniform(())<0.5 or always:\n    #     x = temporal_mask(x)\n    if tf.random.uniform(())<0.5 or always:\n        x = spatial_mask(x)\n    return x\n\ndef get_tfrec_dataset(tfrecords, batch_size=64, max_len=128, target_len=64, teacher_forcing=True, drop_remainder=False, train=False, augment=False, shuffle=False, repeat=False):\n    # Initialize dataset with TFRecords\n    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n    ds = ds.map(lambda x: preprocess(x, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n\n    if train:\n        ds = ds.filter(is_empty)\n\n    if teacher_forcing:\n        ds = ds.map(lambda x,y:((x,y[:-1]),(y[1:],y[1:-1])), tf.data.AUTOTUNE)\n        if augment:\n            ds = ds.map(lambda x,y:((x[0],augment_phrase(x[1])),y), tf.data.AUTOTUNE)\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(shuffle)\n        options = tf.data.Options()\n        options.experimental_deterministic = (False)\n        ds = ds.with_options(options)\n\n    if batch_size:\n        if teacher_forcing:\n            ds = ds.padded_batch(batch_size, padding_values=((PAD,0),(0,0)), padded_shapes=(([max_len,CHANNELS],[target_len,]),([target_len,],[target_len,])), drop_remainder=drop_remainder)\n        else:\n            ds = ds.padded_batch(batch_size, padding_values=(PAD,0), padded_shapes=([max_len,CHANNELS],[target_len,]), drop_remainder=drop_remainder)\n\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n\n    return ds","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.431930Z","iopub.execute_input":"2024-10-06T02:13:44.432216Z","iopub.status.idle":"2024-10-06T02:13:44.462026Z","shell.execute_reply.started":"2024-10-06T02:13:44.432192Z","shell.execute_reply":"2024-10-06T02:13:44.461250Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.ops.gen_dataset_ops import filter_dataset_eager_fallback\nclass ECA(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=5, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.kernel_size = kernel_size\n        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n\n    def call(self, inputs, mask=None):\n        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n        nn = tf.expand_dims(nn, -1)\n        nn = self.conv(nn)\n        nn = tf.squeeze(nn, -1)\n        nn = tf.nn.sigmoid(nn)\n        nn = nn[:,None,:]\n        return inputs * nn\n\nclass MaskingDWConv1D(tf.keras.layers.Layer):\n    '''\n    masked DW1Dconv with strides>1, padding=same.\n    NOTE: padded(masked) frames should always be at the beginning or end of the input sequence.\n    '''\n    def __init__(self, kernel_size, strides=1,\n        dilation_rate=1,\n        padding='same',\n        use_bias=False,\n        kernel_initializer='glorot_uniform',**kwargs):\n        super().__init__(**kwargs)\n        assert padding == 'same' or padding == 'causal'\n        self.strides = strides\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.use_bias = use_bias\n        self.padding = padding\n        self.conv = tf.keras.layers.DepthwiseConv1D(\n                            kernel_size,\n                            strides=strides,\n                            dilation_rate=dilation_rate,\n                            padding=padding,\n                            use_bias=use_bias,\n                            kernel_initializer=kernel_initializer)\n        self.supports_masking = True\n\n    def compute_mask(self, inputs, mask=None):\n      if mask is not None:\n        if self.strides > 1:\n          mask = mask[:,::self.strides]\n      return mask\n\n    def call(self, inputs, mask=None):\n        x = inputs\n        if mask is not None:\n            x = tf.where(mask[...,None], x, tf.constant(0., dtype=x.dtype))\n        x = self.conv(x)\n        return x\n\ndef Conv1DBlock(channel_size,\n          kernel_size,\n          dilation_rate=1,\n          strides=1,\n          drop_rate=0.0,\n          expand_ratio=2,\n          activation='swish',\n          name=None):\n    '''\n    efficient conv1d block, @hoyso48\n    '''\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"conv1dblock\"))\n    # Expansion phase\n    def apply(inputs):\n        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n        channels_expand = channels_in * expand_ratio\n\n        skip = inputs\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'pre_bn')(inputs)\n\n        x = tf.keras.layers.Dense(\n            channels_expand,\n            use_bias=True,\n            activation=activation,\n            name=name + '_expand_conv')(x)\n\n        # Depthwise Convolution\n        x = MaskingDWConv1D(kernel_size,\n            dilation_rate=dilation_rate,\n            strides=strides,\n            use_bias=False,\n            name=name + '_dwconv')(x)\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'conv_bn')(x)\n\n        x = ECA()(x)\n\n        x = tf.keras.layers.Dense(\n            channel_size,\n            use_bias=True,\n            name=name + '_project_conv')(x)\n\n        if drop_rate > 0:\n            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n\n        if (channels_in == channel_size) and (strides == 1):\n            x = tf.keras.layers.add([x, skip], name=name + '_add')\n        return x\n\n    return apply","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.463003Z","iopub.execute_input":"2024-10-06T02:13:44.463275Z","iopub.status.idle":"2024-10-06T02:13:44.479922Z","shell.execute_reply.started":"2024-10-06T02:13:44.463251Z","shell.execute_reply":"2024-10-06T02:13:44.479268Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class PosEmbedding(tf.keras.layers.Layer):\n    def __init__(self, dim=64, max_len=64, **kwargs):\n        super().__init__(**kwargs)\n        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=dim)\n        self.supports_masking = True\n\n    def call(self, x, positions=None):\n        if positions is None:\n            maxlen = tf.shape(x)[1]\n            positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.num_heads = num_heads\n        self.q = tf.keras.layers.Dense(dim, use_bias=False)\n        self.k = tf.keras.layers.Dense(dim, use_bias=False)\n        self.v = tf.keras.layers.Dense(dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n\n    def get_causal_mask(self, q, k):\n        q_len = tf.shape(q)[1]\n        k_len = tf.shape(k)[1]\n        i = tf.range(q_len)[:, None]\n        j = tf.range(k_len)\n        mask = i >= j\n        mask = tf.reshape(mask, (q_len, k_len))\n        return mask\n\n    def merge_input_state(self, input, state, layer):\n        if input is not None and state is not None:\n            return tf.keras.layers.Concatenate(axis=1)([state, layer(input)])\n        elif input is not None and state is None:\n            return layer(input)\n        elif input is None and state is not None:\n            return state\n        else:\n            raise ValueError\n\n    def call(self, q, k=None, v=None, key_state=None, value_state=None, return_states=False, use_causal_mask=False):\n        q = self.q(q)\n        k = self.merge_input_state(k, key_state, self.k)\n        v = self.merge_input_state(v, value_state, self.v)\n        mask = getattr(k, '_keras_mask', None) # we only consider mask from the 'key' here.\n        if mask is not None:\n            mask = mask[:,None,None,:]\n        if use_causal_mask:\n            if mask is not None:\n                mask = tf.logical_and(mask, self.get_causal_mask(q,k)[None,None,:,:])\n            else:\n                mask = self.get_causal_mask(q,k)[None,None,:,:]\n        q_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(q))\n        k_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(k))\n        v_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(v))\n        attn = tf.matmul(q_, k_, transpose_b=True) * self.scale\n\n        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n        attn = self.drop1(attn)\n\n        x = attn @ v_\n        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n        x = self.proj(x)\n        if return_states:\n            return x, k, v\n        else:\n            return x\n\ndef TransformerDecoderBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0., activation='swish', name=None):\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"transformerdecoderblock\"))\n    def apply(q,k,v):\n        x = q\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_self_attn')(x,x,x,use_causal_mask=True)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop1')(x)\n        x = tf.keras.layers.Add(name=name + '_add1')([q, x])\n        attn_out1 = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn2')(x)\n        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_cross_attn')(x,k,v)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop2')(x)\n        x = tf.keras.layers.Add(name=name + '_add2')([attn_out1, x])\n        attn_out2 = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn3')(x)\n        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop3')(x)\n        x = tf.keras.layers.Add(name=name + '_add3')([attn_out2, x])\n        return x\n    return apply\n\n\nclass MultiHeadSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.num_heads = num_heads\n        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        qkv = self.qkv(inputs)\n        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n\n        if mask is not None:\n            mask = mask[:, None, None, :]\n\n        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n        attn = self.drop1(attn)\n\n        x = attn @ v\n        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n        x = self.proj(x)\n        return x\n\ndef TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish', name=None):\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"transformerblock\"))\n    def apply(inputs):\n        x = inputs\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_mhsa')(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop1')(x)\n        x = tf.keras.layers.Add(name=name + '_add1')([inputs, x])\n        attn_out = x\n\n        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'bn2')(x)\n        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop2')(x)\n        x = tf.keras.layers.Add(name=name + '_add2')([attn_out, x])\n        return x\n    return apply","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.482974Z","iopub.execute_input":"2024-10-06T02:13:44.483262Z","iopub.status.idle":"2024-10-06T02:13:44.518548Z","shell.execute_reply.started":"2024-10-06T02:13:44.483238Z","shell.execute_reply":"2024-10-06T02:13:44.517885Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class CTCLoss(tf.keras.losses.Loss):\n    def __init__(self, blank_index=0, input_padding_value=0., target_padding_value=0, **kwargs):\n        super().__init__(**kwargs)\n        self.blank_index = blank_index\n        self.input_padding_value = input_padding_value\n        self.target_padding_value = target_padding_value\n\n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, tf.int32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=tf.int32)\n        label_length = y_true != tf.cast(self.target_padding_value, tf.int32)\n        label_length = tf.reduce_sum(tf.cast(label_length, tf.int32), axis=1, keepdims=False) #(B,)\n        mask = getattr(y_pred, '_keras_mask', None)\n        if mask is not None:\n            input_length = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n        else:\n            input_length = tf.cast(tf.shape(y_pred)[1], dtype=tf.int32)\n            input_length = input_length * tf.ones(shape=(batch_len,), dtype=tf.int32)\n\n        # loss = tf.nn.ctc_loss(y_true, y_pred, label_length=label_length, logit_length=input_length, blank_index=0, logits_time_major=False)\n        loss = classic_ctc_loss(y_true, y_pred, label_length=label_length, logit_length=input_length, blank_index=0) #only for the kaggle TPU\n\n        loss = tf.reduce_mean(loss)\n\n        return loss\n\nclass MaskedSCCE(tf.keras.losses.Loss):\n    def __init__(self, num_classes=NUM_CLASSES, from_logits=True, label_smoothing=0.25, **kwargs):\n        super().__init__(**kwargs)\n        self.num_classes = num_classes\n        self.label_smoothing=label_smoothing\n        self.from_logits = from_logits\n\n    def call(self, y_true, y_pred):\n        mask = y_true!=0\n        N = tf.shape(y_true)[0]\n        y_pred = tf.cast(y_pred, tf.float32)\n        y_true = tf.one_hot(y_true, self.num_classes, axis=-1, dtype=tf.float32)\n        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=self.from_logits, label_smoothing=self.label_smoothing)\n        loss = tf.where(mask, loss, tf.constant(0, dtype=tf.float32))\n        loss = tf.reduce_sum(loss)\n        loss = loss / tf.cast(N, tf.float32)\n        return loss\n\nclass Accuracy(tf.keras.metrics.Metric):\n    def __init__(self, **kwargs):\n        super(Accuracy, self).__init__(name=f'acc', **kwargs)\n        self.acc = tf.keras.metrics.SparseCategoricalAccuracy()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n        mask = y_true != 0\n        y_true = tf.boolean_mask(y_true, mask)\n        y_pred = tf.boolean_mask(y_pred, mask)\n        self.acc.update_state(y_true, y_pred)\n\n    def result(self):\n        return self.acc.result()\n\n    def reset_state(self):\n        self.acc.reset_state()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.519424Z","iopub.execute_input":"2024-10-06T02:13:44.519654Z","iopub.status.idle":"2024-10-06T02:13:44.534142Z","shell.execute_reply.started":"2024-10-06T02:13:44.519634Z","shell.execute_reply":"2024-10-06T02:13:44.533461Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class GreedyDecoder(tf.keras.layers.Layer):\n    def __init__(self, model, max_output_length=64, sos_token_idx=60, eos_token_idx=61, pad_token_idx=0, **kwargs):\n        super().__init__(**kwargs)\n        self.model = model\n        self.encoder = self.model.get_layer('encoder')\n        self.decoder = self.model.get_layer('att_decoder')\n        self.inference_module = self.model.get_layer('att_decoder')\n        self.max_output_length = max_output_length\n        self.sos_token_idx = sos_token_idx\n        self.eos_token_idx = eos_token_idx\n        self.pad_token_idx = pad_token_idx\n\n    def call(self, batch_x):\n        encoder_out = self.encoder(batch_x)\n\n        time = tf.constant(0, dtype=tf.int32)\n        predictions = tf.ones((tf.shape(batch_x)[0],1), dtype=tf.int32) * self.sos_token_idx\n        pad = tf.ones((tf.shape(batch_x)[0],), dtype=tf.int32) * self.pad_token_idx\n        init = True\n\n        def condition(_time, _predictions):\n            return tf.logical_and(_time < self.max_output_length, tf.logical_not(tf.reduce_all(tf.reduce_any(_predictions==self.eos_token_idx, axis=1))))\n\n        def body(_time, _predictions):\n            out = self.inference_module([_predictions, encoder_out])\n            pred_curr = tf.where(tf.reduce_any(_predictions==self.eos_token_idx, axis=1), [self.pad_token_idx], tf.argmax(out[:,-1], axis=-1, output_type=tf.int32))\n            _predictions = tf.concat([_predictions, pred_curr[...,None]], axis=1)\n            return _time+1, _predictions\n\n        _, predictions = tf.while_loop(condition, body, loop_vars=[time, predictions])\n        return predictions[:,1:]\n\nclass KerasCTCDecoder(tf.keras.layers.Layer):\n    def __init__(self, model, greedy=True, beam_width=100, from_logits=True, **kwargs):\n        super().__init__(**kwargs)\n        self.model = model\n        self.greedy = greedy\n        self.beam_width = beam_width\n        self.from_logits = from_logits\n        self.encoder = self.model.get_layer('encoder')\n        self.ctc_decoder = self.model.get_layer('ctc_decoder')\n\n    def call(self, batch_x):\n        encoder_out = self.encoder(batch_x)\n        input_length = tf.reduce_sum(tf.cast(encoder_out._keras_mask, tf.int32), axis=1)\n        predictions = self.ctc_decoder(encoder_out)\n        if not self.greedy and self.from_logits:\n            predictions = tf.nn.softmax(predictions, axis=-1)\n        predictions = tf.keras.backend.ctc_decode(tf.cast(predictions, tf.float32), input_length=input_length, greedy=self.greedy, beam_width=self.beam_width)[0][0]\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.535264Z","iopub.execute_input":"2024-10-06T02:13:44.535530Z","iopub.status.idle":"2024-10-06T02:13:44.548350Z","shell.execute_reply.started":"2024-10-06T02:13:44.535509Z","shell.execute_reply":"2024-10-06T02:13:44.547705Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def make_predictions(recognizer, ds):\n    results = []\n    for batch in tqdm(ds):\n        result = recognizer(batch[0][0])\n        results.append(num_to_char(result.numpy()))\n    results = np.array([item for sublist in results for item in sublist])\n    return results\n\ndef num_to_char(list_of_nums, n2c_dict=NUM_TO_CHAR):\n    def n_to_c(x):\n        return [n2c_dict[a] for a in x if a!=-1]\n    char_list = [''.join(n_to_c(x)).replace('P','').replace('S','').replace('E','') for x in list_of_nums]\n    return np.array(char_list, dtype='str')\n\ndef extract_labels(ds):\n    labels = [num_to_char(x[1][0].numpy()) for x in ds]\n    labels = np.array([item for sublist in labels for item in sublist])\n    return labels\n\nfrom Levenshtein import distance\ndef competition_metric(true, pred):\n    #true: list of strings, ground truths\n    #pred: list of strings, predictions\n    D = sum([distance(x,y) for x,y in zip(true, pred)])\n    N = len(''.join(true))\n    return max((N-D)/N, 0.), D/len(true)\n\ndef display(labels, preds):\n    for target,prediction in zip(labels, preds):\n        print(f\"Target    : {target}\")\n        print(f\"Prediction: {prediction}\")\n        print(\"-\" * 100)\n    return\n\ndef evaluate(model, ds, labels=None, display_index='random', num_display=5):\n    if labels is None:\n        labels = extract_labels(ds)\n    preds = make_predictions(model, ds)\n    score, mean_dist = competition_metric(labels, preds)\n    num_display = min(len(labels), num_display)\n    if display_index=='random':\n        if num_display:\n            idxs = np.random.choice(range(len(labels)),num_display,replace=False)\n            display(labels[idxs], preds[idxs])\n    elif display_index=='init':\n        if num_display:\n            display(labels[:num_display], preds[:num_display])\n    elif isinstance(display_index, list):\n        if display_index:\n            display(labels[display_index], preds[display_index])\n    else:\n        pass\n    print(f'Score: {score:0.4f}')\n    print(f'mean_dist: {mean_dist:0.4f}')\n    # return labels, preds, score\n    del preds, score\n    return\n\nclass Eval(tf.keras.callbacks.Callback):\n    def __init__(self,recognizer,ds,labels=None,eval_epochs=[],display_index='random',num_display=5):\n        super().__init__()\n        self.recognizer = recognizer\n        self.ds = ds\n        self.labels = labels\n        self.eval_epochs = eval_epochs\n        self.display_index = display_index\n        self.num_display = num_display\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch in self.eval_epochs and self.ds is not None: # your custom condition\n            evaluate(self.recognizer, self.ds, self.labels, self.display_index, self.num_display)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.549208Z","iopub.execute_input":"2024-10-06T02:13:44.549435Z","iopub.status.idle":"2024-10-06T02:13:44.563889Z","shell.execute_reply.started":"2024-10-06T02:13:44.549416Z","shell.execute_reply":"2024-10-06T02:13:44.563192Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class AWP(tf.keras.Model):\n    def __init__(self, *args, lr=0.1, eps=1e-6, start_step=0, exclude=[], **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lr = lr\n        self.eps = eps\n        self.start_step = start_step\n        self.exclude = exclude\n\n    def compute_perturbation(self, param, param_gradient):\n        grad = tf.zeros_like(param) + param_gradient\n        #delta = tf.math.divide_no_nan(self.lr * grad * tf.norm(param), tf.norm(grad) + self.eps) #original implemenation from the paper\n        delta = tf.math.divide_no_nan(self.lr * grad, tf.norm(grad) + self.eps)\n        return delta\n\n    def train_step_awp(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        params = self.trainable_variables\n        params_gradients = tape.gradient(loss, self.trainable_variables)\n\n        for i in range(len(params_gradients)):\n            if not any(s in params[i].name for s in self.exclude):\n                delta = self.compute_perturbation(params[i], params_gradients[i])\n                self.trainable_variables[i].assign_add(delta)\n\n        with tf.GradientTape() as tape2:\n            y_pred = self(x, training=True)\n            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n            if hasattr(self.optimizer, 'get_scaled_loss'):\n                new_loss = self.optimizer.get_scaled_loss(new_loss)\n\n        gradients = tape2.gradient(new_loss, self.trainable_variables)\n        if hasattr(self.optimizer, 'get_unscaled_gradients'):\n            gradients =  self.optimizer.get_unscaled_gradients(gradients)\n\n        for i in range(len(params_gradients)):\n            if not any(s in params[i].name for s in self.exclude):\n                delta = self.compute_perturbation(params[i], params_gradients[i])\n                self.trainable_variables[i].assign_sub(delta)\n\n        #if nan is detected, skip update\n        # nan_detected = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients])\n        # _ = tf.cond(nan_detected, lambda:tf.constant(False),lambda:self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)))\n\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        self.compiled_metrics.update_state(y, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n    def train_step(self, data):\n        return tf.cond(self._train_counter < self.start_step, lambda:super(AWP,self).train_step(data), lambda:self.train_step_awp(data))","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.564705Z","iopub.execute_input":"2024-10-06T02:13:44.564940Z","iopub.status.idle":"2024-10-06T02:13:44.578076Z","shell.execute_reply.started":"2024-10-06T02:13:44.564919Z","shell.execute_reply":"2024-10-06T02:13:44.577380Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_model(max_len=128, target_len=64, dim=192, dtype='float32'):\n    ################# ENCODER #################\n    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp1)\n    ksize = 17\n    drop_rate = 0.2\n    x = tf.keras.layers.Dense(dim,use_bias=False,name='stem_conv')(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=0,strides=2)(x) #drop_rate=0 since we don't want to drop the whole output here\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n    x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n\n    encoder = tf.keras.Model(inp1,x,name='encoder')\n\n    ################# CTC DECDODER #################\n    inp3 = tf.keras.Input((x.shape[1],dim),name='ctc_decoder_inp2',dtype=dtype)\n    x = inp3\n    x = tf.keras.layers.RNN(tf.keras.layers.GRUCell(dim), return_sequences=True)(x)\n    x = tf.keras.layers.Dense(dim*2)(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x = tf.keras.layers.Dense(NUM_CLASSES,name='ctc_classifier')(x) #include sos, eos token\n    ctc_decoder = tf.keras.Model(inp3,x,name='ctc_decoder')\n\n    ################# ATT DECODER #################\n    inp2 = tf.keras.Input((None,),name='att_decoder_inp1',dtype='int32')\n    inp3 = tf.keras.Input((x.shape[1],dim),name='att_decoder_inp2',dtype=dtype)\n\n    x = inp3\n    y = tf.keras.layers.Masking(mask_value=0,input_shape=(None,),name='att_decoder_input_masking')(inp2)\n    y = tf.keras.layers.Embedding(NUM_CLASSES,dim,mask_zero=True,name='att_decoder_token_emb')(y) #include sos token\n    y = PosEmbedding(dim,max_len=target_len,name='att_decoder_pos_emb')(y)\n    y = TransformerDecoderBlock(dim,expand=2,num_heads=4,attn_dropout=0.2,name='att_decoder_block1')(y,x,x)\n    y = tf.keras.layers.Dropout(0.5)(y)\n    y = tf.keras.layers.Dense(NUM_CLASSES,name='att_decoder_classifier')(y)\n\n    decoder = tf.keras.Model([inp2,inp3],y,name='att_decoder')\n\n    ################### MODEL #####################\n    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n    inp2 = tf.keras.Input((None,),dtype='int32')\n\n    x = inp1\n    enc_out = encoder(x)\n    y = inp2\n    dec_out = decoder([y, enc_out])\n    ctc_out = ctc_decoder(enc_out)\n    model = tf.keras.Model([inp1,inp2], [dec_out,ctc_out])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.579165Z","iopub.execute_input":"2024-10-06T02:13:44.579409Z","iopub.status.idle":"2024-10-06T02:13:44.597207Z","shell.execute_reply.started":"2024-10-06T02:13:44.579388Z","shell.execute_reply":"2024-10-06T02:13:44.596535Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class CSVLoggerV2(tf.keras.callbacks.CSVLogger):\n    def __init__(self, filename, separator=\",\", resume=0):\n        self.resume = resume\n        super().__init__(filename=filename, separator=separator, append=bool(resume))\n\n    def on_epoch_end(self, epoch, logs=None):\n        super(CSVLoggerV2,self).on_epoch_end(epoch=epoch+self.resume+1, logs=logs)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.598014Z","iopub.execute_input":"2024-10-06T02:13:44.598259Z","iopub.status.idle":"2024-10-06T02:13:44.608891Z","shell.execute_reply.started":"2024-10-06T02:13:44.598237Z","shell.execute_reply":"2024-10-06T02:13:44.608241Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def train_fold(CFG, fold, train_files, valid_files=None, strategy=STRATEGY, summary=True):\n    seed_everything(CFG.seed)\n    tf.keras.backend.clear_session()\n    gc.collect()\n    # tf.config.optimizer.set_jit(True)\n\n    policy = mixed_precision.Policy(CFG.policy)\n    mixed_precision.set_global_policy(policy)\n\n    if CFG.resume == 'auto':\n        if os.path.isfile(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv'):\n            resume = pd.read_csv(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')['epoch'].values[-1]\n            resume = 0 if resume == CFG.epoch else resume #restart if training is already fininshed\n        else:\n            resume = 0\n    else:\n        resume = CFG.resume\n\n    if fold != 'all':\n        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.train_batch_size, max_len=CFG.max_len, drop_remainder=True, train=True, augment=True, repeat=True, shuffle=4096)\n        valid_ds = get_tfrec_dataset(valid_files, batch_size=CFG.valid_batch_size, max_len=CFG.max_len, drop_remainder=True, repeat=False, shuffle=False)\n    else:\n        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.train_batch_size, max_len=CFG.max_len, drop_remainder=True, train=True, augment=True, repeat=True, shuffle=4096)\n        valid_ds = None\n        valid_files = []\n\n    num_train = count_data_items(train_files)\n    num_valid = count_data_items(valid_files)\n    steps_per_epoch = num_train//CFG.train_batch_size\n    with strategy.scope():\n        model = get_model(max_len=CFG.max_len, dim=CFG.dim, dtype='bfloat16') #dtype should be matched with CFG.policy\n\n        schedule = OneCycleLR(CFG.lr, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min, decay_type=CFG.decay_type, warmup_type=CFG.warmup_type)\n        decay_schedule = OneCycleLR(CFG.lr*CFG.weight_decay, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min*CFG.weight_decay, decay_type=CFG.decay_type, warmup_type=CFG.warmup_type)\n\n        awp_start_epoch = max(CFG.awp_start_epoch - resume, 0)\n        awp_step = int(awp_start_epoch * steps_per_epoch)\n        if CFG.fgm:\n            model = FGM(model.input, model.output, lr=CFG.awp_lr, eps=0., start_step=awp_step)\n        elif CFG.awp:\n            model = AWP(model.input, model.output, lr=CFG.awp_lr, eps=0., start_step=awp_step, exclude=['bias','gamma','beta','rnn'])\n\n        # opt = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4)\n        # opt = tfa.optimizers.Lookahead(opt,sync_period=5)\n        opt = tfa.optimizers.AdamW(learning_rate=schedule, weight_decay=decay_schedule)\n\n        model.compile(\n            optimizer=opt,\n            loss=[MaskedSCCE(label_smoothing=0.25), CTCLoss()],\n            loss_weights=[0.75,0.25],\n            metrics=[\n                [\n                Accuracy(),\n                ],\n                [],\n            ],\n        )\n\n    if summary:\n        print()\n        model.summary()\n        print()\n        print(train_ds, valid_ds)\n        print()\n        schedule.plot()\n        print()\n        init=False\n    print(f'---------fold{fold}---------')\n    print(f'train:{num_train} valid:{num_valid}')\n    print()\n\n    if resume:\n        print(f'resume from epoch{resume}')\n        if CFG.resume_ckpt:\n            print(f'load weights from {CFG.resume_ckpt}')\n            model.load_weights(CFG.resume_ckpt)\n        else:\n            print(f'load weights from {CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n            model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n        # if train_ds is not None:\n        #     model.evaluate(train_ds.take(steps_per_epoch))\n        # if valid_ds is not None:\n        #     model.evaluate(valid_ds)\n\n    logger = CSVLoggerV2(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv', resume=resume)\n\n    mode = 'min'\n    if fold != 'all':\n        monitor = 'val_loss'\n    else:\n        monitor = 'loss'\n    if resume:\n        prev_best = pd.read_csv(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')[monitor].agg(mode)\n    else:\n        prev_best = None\n    sv_loss = tf.keras.callbacks.ModelCheckpoint(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5', monitor=monitor, verbose=0, save_best_only=True,\n                  save_weights_only=True, mode='min', save_freq='epoch', initial_value_threshold=prev_best)\n\n    snap = Snapshot(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', snapshot_epochs=[])\n    # swa = SWA(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.swa_epochs, strategy=strategy, train_ds=train_ds, valid_ds=valid_ds)\n\n    callbacks = []\n    if CFG.save_output:\n        callbacks.append(logger)\n        callbacks.append(snap)\n        # callbacks.append(swa)\n        callbacks.append(sv_loss)\n\n    history = model.fit(\n        train_ds,\n        epochs=CFG.epoch-resume,\n        steps_per_epoch=steps_per_epoch,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        verbose=CFG.verbose,\n    )\n\n    if fold != 'all':\n        ds = get_tfrec_dataset(valid_files, batch_size=CFG.valid_batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n        labels = extract_labels(ds)\n        model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n        print('ATTENTION EVAL')\n        evaluate(GreedyDecoder(model),ds,labels)\n        print()\n        print('CTC EVAL')\n        evaluate(KerasCTCDecoder(model, greedy=True),ds,labels)\n\n    return model, history\n\ndef train_folds(CFG, folds, strategy=STRATEGY, summary=True):\n    for fold in folds:\n        if fold != 'all':\n            all_files = TRAIN_FILENAMES\n            train_files = [x for x in all_files if f'fold{fold}' not in x]\n            valid_files = [x for x in all_files if f'fold{fold}' in x]\n        else:\n            train_files = TRAIN_FILENAMES\n            valid_files = None\n\n        train_fold(CFG, fold, train_files, valid_files, strategy=strategy, summary=summary)\n    return","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.609994Z","iopub.execute_input":"2024-10-06T02:13:44.610253Z","iopub.status.idle":"2024-10-06T02:13:44.632640Z","shell.execute_reply.started":"2024-10-06T02:13:44.610231Z","shell.execute_reply":"2024-10-06T02:13:44.631848Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    n_splits = 5\n    save_output = True\n    output_dir = '.'\n\n    seed = 42\n    verbose = 'auto' #0) silent 1) progress bar 2) one line per epoch\n\n    dim = 192\n    max_len = 768\n\n    policy = 'mixed_bfloat16' #'float32') fp32, 'mixed_float16') GPU+fp16, 'mixed_bfloat16') TPU+fp16\n    replicas = N_REPLICAS\n    lr = 5e-4 * replicas\n    weight_decay = 0.01\n    lr_min = 1e-6\n    epoch = 100 #400\n    warmup = 0.1\n    warmup_type = 'linear'\n    decay_type = 'cosine'\n    train_batch_size = 16 * replicas\n    valid_batch_size = 64 * replicas\n\n    fgm = False\n    awp = False #True\n    awp_lr = 0.2\n    awp_start_epoch = 0.1 * epoch\n\n    resume = 0\n    resume_ckpt = ''\n    comment =  f'aslfr-fp16-192d-17l-ctcattjoint-seed{seed}'","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.633556Z","iopub.execute_input":"2024-10-06T02:13:44.633770Z","iopub.status.idle":"2024-10-06T02:13:44.644277Z","shell.execute_reply.started":"2024-10-06T02:13:44.633751Z","shell.execute_reply":"2024-10-06T02:13:44.643594Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"ds = get_tfrec_dataset(TRAIN_FILENAMES, train=True, augment=True, batch_size=4, shuffle=4)\nprint(ds)\nfor x in ds:\n    temp_train = x\n    break\n\ncount = 0\n\ntry:\n    for x in ds:\n        count += 1\nexcept:\n    print('there is corrupted data')\n\nprint(count)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:44.645076Z","iopub.execute_input":"2024-10-06T02:13:44.645309Z","iopub.status.idle":"2024-10-06T02:13:52.125259Z","shell.execute_reply.started":"2024-10-06T02:13:44.645289Z","shell.execute_reply":"2024-10-06T02:13:52.124273Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 128, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(None, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(None, 64), dtype=tf.int32, name=None), TensorSpec(shape=(None, 64), dtype=tf.int32, name=None)))>\n984\n","output_type":"stream"}]},{"cell_type":"code","source":"model = get_model()\n\n# y = model(temp_train[0], training=True)\n# model.load_weights('/kaggle/input/asl-model/model.h5')\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:52.126427Z","iopub.execute_input":"2024-10-06T02:13:52.126722Z","iopub.status.idle":"2024-10-06T02:13:57.421744Z","shell.execute_reply.started":"2024-10-06T02:13:52.126697Z","shell.execute_reply":"2024-10-06T02:13:57.420661Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 128, 1629)]  0           []                               \n                                                                                                  \n input_3 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n encoder (Functional)           (None, 64, 192)      5565377     ['input_2[0][0]']                \n                                                                                                  \n att_decoder (Functional)       (None, None, 62)     480830      ['input_3[0][0]',                \n                                                                  'encoder[0][0]']                \n                                                                                                  \n ctc_decoder (Functional)       (None, 64, 62)       320318      ['encoder[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 6,366,525\nTrainable params: 6,336,957\nNon-trainable params: 29,568\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"#check supports_masking\nfor x in model.layers:\n    if not x.supports_masking:\n        print(x.supports_masking, x.name)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:57.423058Z","iopub.execute_input":"2024-10-06T02:13:57.423384Z","iopub.status.idle":"2024-10-06T02:13:57.428075Z","shell.execute_reply.started":"2024-10-06T02:13:57.423356Z","shell.execute_reply":"2024-10-06T02:13:57.427193Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"CFG.resume = 'auto'\ntrain_folds(CFG, [0])","metadata":{"execution":{"iopub.status.busy":"2024-10-06T02:13:57.429067Z","iopub.execute_input":"2024-10-06T02:13:57.429323Z","iopub.status.idle":"2024-10-06T02:52:32.078669Z","shell.execute_reply.started":"2024-10-06T02:13:57.429302Z","shell.execute_reply":"2024-10-06T02:52:32.077572Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 768, 1629)]  0           []                               \n                                                                                                  \n input_3 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n encoder (Functional)           (None, 384, 192)     5565377     ['input_2[0][0]']                \n                                                                                                  \n att_decoder (Functional)       (None, None, 62)     480830      ['input_3[0][0]',                \n                                                                  'encoder[0][0]']                \n                                                                                                  \n ctc_decoder (Functional)       (None, 384, 62)      320318      ['encoder[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 6,366,525\nTrainable params: 6,336,957\nNon-trainable params: 29,568\n__________________________________________________________________________________________________\n\n<_PrefetchDataset element_spec=((TensorSpec(shape=(128, 768, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(128, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(128, 64), dtype=tf.int32, name=None), TensorSpec(shape=(128, 64), dtype=tf.int32, name=None)))> <_PrefetchDataset element_spec=((TensorSpec(shape=(512, 768, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(512, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(512, 64), dtype=tf.int32, name=None), TensorSpec(shape=(512, 64), dtype=tf.int32, name=None)))>\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\n---------fold0---------\ntrain:2523 valid:1410\n\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2024-10-06 02:16:01.103158: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AdamW/AdamW/AssignAddVariableOp.\n2024-10-06 02:16:05.051386: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AdamW/AdamW/AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"19/19 [==============================] - ETA: 0s - loss: 237.2706 - att_decoder_loss: 47.4898 - ctc_decoder_loss: 806.6132 - att_decoder_acc: 0.0272","output_type":"stream"},{"name":"stderr","text":"2024-10-06 02:17:40.223486: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2024-10-06 02:17:40.414680: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"19/19 [==============================] - 224s 2s/step - loss: 237.2706 - att_decoder_loss: 47.4898 - ctc_decoder_loss: 806.6131 - att_decoder_acc: 0.0272 - val_loss: 121.3848 - val_att_decoder_loss: 35.6154 - val_ctc_decoder_loss: 378.6929 - val_att_decoder_acc: 0.1277\nEpoch 2/100\n19/19 [==============================] - 17s 940ms/step - loss: 45.6546 - att_decoder_loss: 35.0712 - ctc_decoder_loss: 77.4047 - att_decoder_acc: 0.1420 - val_loss: 34.5547 - val_att_decoder_loss: 32.4069 - val_ctc_decoder_loss: 40.9980 - val_att_decoder_acc: 0.1135\nEpoch 3/100\n19/19 [==============================] - 15s 830ms/step - loss: 34.1690 - att_decoder_loss: 31.3369 - ctc_decoder_loss: 42.6653 - att_decoder_acc: 0.2035 - val_loss: 33.4962 - val_att_decoder_loss: 31.3671 - val_ctc_decoder_loss: 39.8834 - val_att_decoder_acc: 0.1493\nEpoch 4/100\n19/19 [==============================] - 14s 765ms/step - loss: 31.1791 - att_decoder_loss: 29.3785 - ctc_decoder_loss: 36.5811 - att_decoder_acc: 0.2726 - val_loss: 32.5291 - val_att_decoder_loss: 30.6308 - val_ctc_decoder_loss: 38.2238 - val_att_decoder_acc: 0.2578\nEpoch 5/100\n19/19 [==============================] - 14s 755ms/step - loss: 29.0685 - att_decoder_loss: 27.7866 - ctc_decoder_loss: 32.9142 - att_decoder_acc: 0.3457 - val_loss: 29.8690 - val_att_decoder_loss: 28.5555 - val_ctc_decoder_loss: 33.8093 - val_att_decoder_acc: 0.4194\nEpoch 6/100\n19/19 [==============================] - 14s 751ms/step - loss: 27.4003 - att_decoder_loss: 26.5911 - ctc_decoder_loss: 29.8278 - att_decoder_acc: 0.3939 - val_loss: 25.4546 - val_att_decoder_loss: 24.6164 - val_ctc_decoder_loss: 27.9693 - val_att_decoder_acc: 0.6149\nEpoch 7/100\n19/19 [==============================] - 13s 735ms/step - loss: 26.4678 - att_decoder_loss: 25.9443 - ctc_decoder_loss: 28.0383 - att_decoder_acc: 0.4279 - val_loss: 22.6244 - val_att_decoder_loss: 21.3909 - val_ctc_decoder_loss: 26.3248 - val_att_decoder_acc: 0.6950\nEpoch 8/100\n19/19 [==============================] - 14s 749ms/step - loss: 25.9911 - att_decoder_loss: 25.6504 - ctc_decoder_loss: 27.0131 - att_decoder_acc: 0.4452 - val_loss: 21.7729 - val_att_decoder_loss: 20.3936 - val_ctc_decoder_loss: 25.9110 - val_att_decoder_acc: 0.7193\nEpoch 9/100\n19/19 [==============================] - 13s 737ms/step - loss: 25.4100 - att_decoder_loss: 25.0820 - ctc_decoder_loss: 26.3942 - att_decoder_acc: 0.4730 - val_loss: 21.1056 - val_att_decoder_loss: 19.6806 - val_ctc_decoder_loss: 25.3806 - val_att_decoder_acc: 0.7605\nEpoch 10/100\n19/19 [==============================] - 14s 746ms/step - loss: 24.9978 - att_decoder_loss: 24.7053 - ctc_decoder_loss: 25.8753 - att_decoder_acc: 0.4797 - val_loss: 21.0179 - val_att_decoder_loss: 19.5415 - val_ctc_decoder_loss: 25.4472 - val_att_decoder_acc: 0.7649\nEpoch 11/100\n19/19 [==============================] - 14s 745ms/step - loss: 24.8446 - att_decoder_loss: 24.5061 - ctc_decoder_loss: 25.8603 - att_decoder_acc: 0.4899 - val_loss: 20.6099 - val_att_decoder_loss: 19.1926 - val_ctc_decoder_loss: 24.8616 - val_att_decoder_acc: 0.7841\nEpoch 12/100\n19/19 [==============================] - 13s 711ms/step - loss: 24.7624 - att_decoder_loss: 24.4344 - ctc_decoder_loss: 25.7464 - att_decoder_acc: 0.5098 - val_loss: 20.6263 - val_att_decoder_loss: 19.0451 - val_ctc_decoder_loss: 25.3697 - val_att_decoder_acc: 0.7810\nEpoch 13/100\n19/19 [==============================] - 14s 752ms/step - loss: 24.2089 - att_decoder_loss: 23.8467 - ctc_decoder_loss: 25.2952 - att_decoder_acc: 0.5233 - val_loss: 20.4068 - val_att_decoder_loss: 18.9174 - val_ctc_decoder_loss: 24.8752 - val_att_decoder_acc: 0.7884\nEpoch 14/100\n19/19 [==============================] - 14s 753ms/step - loss: 24.2616 - att_decoder_loss: 23.8048 - ctc_decoder_loss: 25.6318 - att_decoder_acc: 0.5404 - val_loss: 20.2575 - val_att_decoder_loss: 18.6863 - val_ctc_decoder_loss: 24.9712 - val_att_decoder_acc: 0.7913\nEpoch 15/100\n19/19 [==============================] - 13s 741ms/step - loss: 23.6510 - att_decoder_loss: 23.2202 - ctc_decoder_loss: 24.9435 - att_decoder_acc: 0.5497 - val_loss: 20.2279 - val_att_decoder_loss: 18.6660 - val_ctc_decoder_loss: 24.9135 - val_att_decoder_acc: 0.7927\nEpoch 16/100\n19/19 [==============================] - 13s 717ms/step - loss: 23.3913 - att_decoder_loss: 22.9604 - ctc_decoder_loss: 24.6840 - att_decoder_acc: 0.5480 - val_loss: 20.3036 - val_att_decoder_loss: 18.6307 - val_ctc_decoder_loss: 25.3225 - val_att_decoder_acc: 0.7969\nEpoch 17/100\n19/19 [==============================] - 14s 763ms/step - loss: 23.6677 - att_decoder_loss: 23.1860 - ctc_decoder_loss: 25.1130 - att_decoder_acc: 0.5612 - val_loss: 20.1441 - val_att_decoder_loss: 18.5443 - val_ctc_decoder_loss: 24.9434 - val_att_decoder_acc: 0.8019\nEpoch 18/100\n19/19 [==============================] - 13s 737ms/step - loss: 23.5260 - att_decoder_loss: 23.0015 - ctc_decoder_loss: 25.0993 - att_decoder_acc: 0.5701 - val_loss: 20.0102 - val_att_decoder_loss: 18.4077 - val_ctc_decoder_loss: 24.8176 - val_att_decoder_acc: 0.8008\nEpoch 19/100\n19/19 [==============================] - 13s 745ms/step - loss: 23.1742 - att_decoder_loss: 22.6306 - ctc_decoder_loss: 24.8053 - att_decoder_acc: 0.5766 - val_loss: 20.0057 - val_att_decoder_loss: 18.5094 - val_ctc_decoder_loss: 24.4947 - val_att_decoder_acc: 0.7961\nEpoch 20/100\n19/19 [==============================] - 13s 744ms/step - loss: 23.1465 - att_decoder_loss: 22.6112 - ctc_decoder_loss: 24.7524 - att_decoder_acc: 0.5827 - val_loss: 19.8888 - val_att_decoder_loss: 18.3226 - val_ctc_decoder_loss: 24.5873 - val_att_decoder_acc: 0.7998\nEpoch 21/100\n19/19 [==============================] - 13s 739ms/step - loss: 23.1237 - att_decoder_loss: 22.5619 - ctc_decoder_loss: 24.8091 - att_decoder_acc: 0.5897 - val_loss: 19.8140 - val_att_decoder_loss: 18.2611 - val_ctc_decoder_loss: 24.4728 - val_att_decoder_acc: 0.8072\nEpoch 22/100\n19/19 [==============================] - 13s 698ms/step - loss: 22.9455 - att_decoder_loss: 22.3219 - ctc_decoder_loss: 24.8164 - att_decoder_acc: 0.6011 - val_loss: 19.8825 - val_att_decoder_loss: 18.2746 - val_ctc_decoder_loss: 24.7062 - val_att_decoder_acc: 0.8067\nEpoch 23/100\n19/19 [==============================] - 13s 735ms/step - loss: 22.8597 - att_decoder_loss: 22.2587 - ctc_decoder_loss: 24.6630 - att_decoder_acc: 0.6049 - val_loss: 19.7443 - val_att_decoder_loss: 18.2457 - val_ctc_decoder_loss: 24.2400 - val_att_decoder_acc: 0.8063\nEpoch 24/100\n19/19 [==============================] - 13s 704ms/step - loss: 22.5409 - att_decoder_loss: 21.9280 - ctc_decoder_loss: 24.3794 - att_decoder_acc: 0.6042 - val_loss: 19.7994 - val_att_decoder_loss: 18.2497 - val_ctc_decoder_loss: 24.4485 - val_att_decoder_acc: 0.8073\nEpoch 25/100\n19/19 [==============================] - 14s 767ms/step - loss: 22.6645 - att_decoder_loss: 22.0253 - ctc_decoder_loss: 24.5823 - att_decoder_acc: 0.6197 - val_loss: 19.6641 - val_att_decoder_loss: 18.1502 - val_ctc_decoder_loss: 24.2059 - val_att_decoder_acc: 0.8135\nEpoch 26/100\n19/19 [==============================] - 13s 705ms/step - loss: 22.8178 - att_decoder_loss: 22.1760 - ctc_decoder_loss: 24.7433 - att_decoder_acc: 0.6201 - val_loss: 19.7240 - val_att_decoder_loss: 18.1932 - val_ctc_decoder_loss: 24.3165 - val_att_decoder_acc: 0.8154\nEpoch 27/100\n19/19 [==============================] - 14s 747ms/step - loss: 22.4810 - att_decoder_loss: 21.8459 - ctc_decoder_loss: 24.3861 - att_decoder_acc: 0.6255 - val_loss: 19.5961 - val_att_decoder_loss: 18.1190 - val_ctc_decoder_loss: 24.0274 - val_att_decoder_acc: 0.8102\nEpoch 28/100\n19/19 [==============================] - 13s 712ms/step - loss: 22.2270 - att_decoder_loss: 21.6288 - ctc_decoder_loss: 24.0218 - att_decoder_acc: 0.6203 - val_loss: 19.7040 - val_att_decoder_loss: 18.2070 - val_ctc_decoder_loss: 24.1953 - val_att_decoder_acc: 0.8154\nEpoch 29/100\n19/19 [==============================] - 13s 713ms/step - loss: 22.5211 - att_decoder_loss: 21.9083 - ctc_decoder_loss: 24.3598 - att_decoder_acc: 0.6303 - val_loss: 19.8892 - val_att_decoder_loss: 18.2124 - val_ctc_decoder_loss: 24.9194 - val_att_decoder_acc: 0.8082\nEpoch 30/100\n19/19 [==============================] - 14s 753ms/step - loss: 22.1348 - att_decoder_loss: 21.5255 - ctc_decoder_loss: 23.9627 - att_decoder_acc: 0.6336 - val_loss: 19.5534 - val_att_decoder_loss: 18.0706 - val_ctc_decoder_loss: 24.0018 - val_att_decoder_acc: 0.8140\nEpoch 31/100\n19/19 [==============================] - 14s 756ms/step - loss: 22.1250 - att_decoder_loss: 21.4840 - ctc_decoder_loss: 24.0482 - att_decoder_acc: 0.6460 - val_loss: 19.4427 - val_att_decoder_loss: 17.9422 - val_ctc_decoder_loss: 23.9444 - val_att_decoder_acc: 0.8181\nEpoch 32/100\n19/19 [==============================] - 14s 748ms/step - loss: 22.0455 - att_decoder_loss: 21.4698 - ctc_decoder_loss: 23.7726 - att_decoder_acc: 0.6405 - val_loss: 19.4183 - val_att_decoder_loss: 18.0840 - val_ctc_decoder_loss: 23.4211 - val_att_decoder_acc: 0.8218\nEpoch 33/100\n19/19 [==============================] - 13s 698ms/step - loss: 22.0279 - att_decoder_loss: 21.4361 - ctc_decoder_loss: 23.8033 - att_decoder_acc: 0.6481 - val_loss: 19.5320 - val_att_decoder_loss: 18.0131 - val_ctc_decoder_loss: 24.0888 - val_att_decoder_acc: 0.8187\nEpoch 34/100\n19/19 [==============================] - 13s 711ms/step - loss: 21.6305 - att_decoder_loss: 21.0361 - ctc_decoder_loss: 23.4136 - att_decoder_acc: 0.6575 - val_loss: 19.4213 - val_att_decoder_loss: 17.8913 - val_ctc_decoder_loss: 24.0112 - val_att_decoder_acc: 0.8215\nEpoch 35/100\n19/19 [==============================] - 13s 734ms/step - loss: 21.9159 - att_decoder_loss: 21.2918 - ctc_decoder_loss: 23.7883 - att_decoder_acc: 0.6646 - val_loss: 19.0835 - val_att_decoder_loss: 17.8266 - val_ctc_decoder_loss: 22.8542 - val_att_decoder_acc: 0.8284\nEpoch 36/100\n19/19 [==============================] - 14s 748ms/step - loss: 21.6332 - att_decoder_loss: 21.0555 - ctc_decoder_loss: 23.3661 - att_decoder_acc: 0.6665 - val_loss: 19.0485 - val_att_decoder_loss: 17.6836 - val_ctc_decoder_loss: 23.1431 - val_att_decoder_acc: 0.8374\nEpoch 37/100\n19/19 [==============================] - 13s 744ms/step - loss: 21.7783 - att_decoder_loss: 21.1939 - ctc_decoder_loss: 23.5315 - att_decoder_acc: 0.6781 - val_loss: 18.9458 - val_att_decoder_loss: 17.7085 - val_ctc_decoder_loss: 22.6578 - val_att_decoder_acc: 0.8387\nEpoch 38/100\n19/19 [==============================] - 17s 912ms/step - loss: 21.2141 - att_decoder_loss: 20.7003 - ctc_decoder_loss: 22.7554 - att_decoder_acc: 0.6758 - val_loss: 18.9015 - val_att_decoder_loss: 17.6704 - val_ctc_decoder_loss: 22.5950 - val_att_decoder_acc: 0.8519\nEpoch 39/100\n19/19 [==============================] - 14s 767ms/step - loss: 21.4124 - att_decoder_loss: 20.8916 - ctc_decoder_loss: 22.9748 - att_decoder_acc: 0.6808 - val_loss: 18.8395 - val_att_decoder_loss: 17.5772 - val_ctc_decoder_loss: 22.6266 - val_att_decoder_acc: 0.8467\nEpoch 40/100\n19/19 [==============================] - 14s 766ms/step - loss: 20.8194 - att_decoder_loss: 20.3386 - ctc_decoder_loss: 22.2621 - att_decoder_acc: 0.6919 - val_loss: 18.6422 - val_att_decoder_loss: 17.4934 - val_ctc_decoder_loss: 22.0887 - val_att_decoder_acc: 0.8607\nEpoch 41/100\n19/19 [==============================] - 14s 747ms/step - loss: 20.7213 - att_decoder_loss: 20.2188 - ctc_decoder_loss: 22.2291 - att_decoder_acc: 0.7065 - val_loss: 18.5035 - val_att_decoder_loss: 17.3817 - val_ctc_decoder_loss: 21.8689 - val_att_decoder_acc: 0.8599\nEpoch 42/100\n19/19 [==============================] - 14s 760ms/step - loss: 21.0418 - att_decoder_loss: 20.5278 - ctc_decoder_loss: 22.5838 - att_decoder_acc: 0.7163 - val_loss: 18.3965 - val_att_decoder_loss: 17.2606 - val_ctc_decoder_loss: 21.8041 - val_att_decoder_acc: 0.8750\nEpoch 43/100\n19/19 [==============================] - 13s 721ms/step - loss: 20.5171 - att_decoder_loss: 20.1049 - ctc_decoder_loss: 21.7537 - att_decoder_acc: 0.7139 - val_loss: 19.1850 - val_att_decoder_loss: 17.9051 - val_ctc_decoder_loss: 23.0246 - val_att_decoder_acc: 0.8496\nEpoch 44/100\n19/19 [==============================] - 14s 748ms/step - loss: 20.5468 - att_decoder_loss: 20.1370 - ctc_decoder_loss: 21.7761 - att_decoder_acc: 0.7237 - val_loss: 18.2261 - val_att_decoder_loss: 17.2513 - val_ctc_decoder_loss: 21.1506 - val_att_decoder_acc: 0.8748\nEpoch 45/100\n19/19 [==============================] - 13s 713ms/step - loss: 20.2649 - att_decoder_loss: 19.9116 - ctc_decoder_loss: 21.3249 - att_decoder_acc: 0.7327 - val_loss: 18.3546 - val_att_decoder_loss: 17.2968 - val_ctc_decoder_loss: 21.5283 - val_att_decoder_acc: 0.8724\nEpoch 46/100\n19/19 [==============================] - 14s 758ms/step - loss: 19.9818 - att_decoder_loss: 19.6898 - ctc_decoder_loss: 20.8579 - att_decoder_acc: 0.7461 - val_loss: 17.9471 - val_att_decoder_loss: 16.9361 - val_ctc_decoder_loss: 20.9801 - val_att_decoder_acc: 0.8840\nEpoch 47/100\n19/19 [==============================] - 13s 705ms/step - loss: 19.9974 - att_decoder_loss: 19.7147 - ctc_decoder_loss: 20.8455 - att_decoder_acc: 0.7449 - val_loss: 18.0902 - val_att_decoder_loss: 17.1014 - val_ctc_decoder_loss: 21.0565 - val_att_decoder_acc: 0.8888\nEpoch 48/100\n19/19 [==============================] - 14s 752ms/step - loss: 19.9412 - att_decoder_loss: 19.7047 - ctc_decoder_loss: 20.6508 - att_decoder_acc: 0.7442 - val_loss: 17.5863 - val_att_decoder_loss: 16.8425 - val_ctc_decoder_loss: 19.8175 - val_att_decoder_acc: 0.8994\nEpoch 49/100\n19/19 [==============================] - 14s 754ms/step - loss: 19.5404 - att_decoder_loss: 19.3830 - ctc_decoder_loss: 20.0127 - att_decoder_acc: 0.7697 - val_loss: 17.5066 - val_att_decoder_loss: 16.9146 - val_ctc_decoder_loss: 19.2826 - val_att_decoder_acc: 0.9064\nEpoch 50/100\n19/19 [==============================] - 14s 752ms/step - loss: 19.4122 - att_decoder_loss: 19.3115 - ctc_decoder_loss: 19.7142 - att_decoder_acc: 0.7758 - val_loss: 17.4658 - val_att_decoder_loss: 16.7714 - val_ctc_decoder_loss: 19.5489 - val_att_decoder_acc: 0.9052\nEpoch 51/100\n19/19 [==============================] - 14s 755ms/step - loss: 19.0508 - att_decoder_loss: 18.9815 - ctc_decoder_loss: 19.2588 - att_decoder_acc: 0.7888 - val_loss: 16.9547 - val_att_decoder_loss: 16.4507 - val_ctc_decoder_loss: 18.4667 - val_att_decoder_acc: 0.9242\nEpoch 52/100\n19/19 [==============================] - 13s 697ms/step - loss: 18.8549 - att_decoder_loss: 18.9008 - ctc_decoder_loss: 18.7174 - att_decoder_acc: 0.7901 - val_loss: 17.0793 - val_att_decoder_loss: 16.6819 - val_ctc_decoder_loss: 18.2716 - val_att_decoder_acc: 0.9279\nEpoch 53/100\n19/19 [==============================] - 14s 751ms/step - loss: 18.5447 - att_decoder_loss: 18.6543 - ctc_decoder_loss: 18.2157 - att_decoder_acc: 0.7997 - val_loss: 16.8614 - val_att_decoder_loss: 16.4718 - val_ctc_decoder_loss: 18.0302 - val_att_decoder_acc: 0.9262\nEpoch 54/100\n19/19 [==============================] - 14s 754ms/step - loss: 18.5584 - att_decoder_loss: 18.6987 - ctc_decoder_loss: 18.1373 - att_decoder_acc: 0.8168 - val_loss: 16.6333 - val_att_decoder_loss: 16.2705 - val_ctc_decoder_loss: 17.7215 - val_att_decoder_acc: 0.9295\nEpoch 55/100\n19/19 [==============================] - 13s 700ms/step - loss: 18.2545 - att_decoder_loss: 18.4275 - ctc_decoder_loss: 17.7357 - att_decoder_acc: 0.8229 - val_loss: 16.7506 - val_att_decoder_loss: 16.3434 - val_ctc_decoder_loss: 17.9721 - val_att_decoder_acc: 0.9278\nEpoch 56/100\n19/19 [==============================] - 14s 749ms/step - loss: 18.1670 - att_decoder_loss: 18.4487 - ctc_decoder_loss: 17.3221 - att_decoder_acc: 0.8264 - val_loss: 16.2600 - val_att_decoder_loss: 16.1569 - val_ctc_decoder_loss: 16.5693 - val_att_decoder_acc: 0.9422\nEpoch 57/100\n19/19 [==============================] - 13s 703ms/step - loss: 17.7600 - att_decoder_loss: 18.0811 - ctc_decoder_loss: 16.7967 - att_decoder_acc: 0.8378 - val_loss: 16.2737 - val_att_decoder_loss: 16.2488 - val_ctc_decoder_loss: 16.3483 - val_att_decoder_acc: 0.9393\nEpoch 58/100\n19/19 [==============================] - 13s 740ms/step - loss: 17.5470 - att_decoder_loss: 18.0133 - ctc_decoder_loss: 16.1482 - att_decoder_acc: 0.8435 - val_loss: 16.0805 - val_att_decoder_loss: 16.1481 - val_ctc_decoder_loss: 15.8778 - val_att_decoder_acc: 0.9404\nEpoch 59/100\n19/19 [==============================] - 13s 739ms/step - loss: 17.4546 - att_decoder_loss: 18.0013 - ctc_decoder_loss: 15.8148 - att_decoder_acc: 0.8505 - val_loss: 15.5547 - val_att_decoder_loss: 15.9614 - val_ctc_decoder_loss: 14.3347 - val_att_decoder_acc: 0.9560\nEpoch 60/100\n19/19 [==============================] - 13s 706ms/step - loss: 17.1748 - att_decoder_loss: 17.8242 - ctc_decoder_loss: 15.2266 - att_decoder_acc: 0.8610 - val_loss: 15.6797 - val_att_decoder_loss: 16.1196 - val_ctc_decoder_loss: 14.3598 - val_att_decoder_acc: 0.9490\nEpoch 61/100\n19/19 [==============================] - 14s 749ms/step - loss: 16.9888 - att_decoder_loss: 17.7028 - ctc_decoder_loss: 14.8467 - att_decoder_acc: 0.8633 - val_loss: 15.3756 - val_att_decoder_loss: 15.9773 - val_ctc_decoder_loss: 13.5706 - val_att_decoder_acc: 0.9539\nEpoch 62/100\n19/19 [==============================] - 14s 747ms/step - loss: 16.7122 - att_decoder_loss: 17.5311 - ctc_decoder_loss: 14.2555 - att_decoder_acc: 0.8603 - val_loss: 15.3314 - val_att_decoder_loss: 16.0201 - val_ctc_decoder_loss: 13.2651 - val_att_decoder_acc: 0.9594\nEpoch 63/100\n19/19 [==============================] - 13s 709ms/step - loss: 16.5470 - att_decoder_loss: 17.4927 - ctc_decoder_loss: 13.7099 - att_decoder_acc: 0.8783 - val_loss: 15.7624 - val_att_decoder_loss: 16.0950 - val_ctc_decoder_loss: 14.7648 - val_att_decoder_acc: 0.9384\nEpoch 64/100\n19/19 [==============================] - 13s 741ms/step - loss: 16.4836 - att_decoder_loss: 17.4904 - ctc_decoder_loss: 13.4631 - att_decoder_acc: 0.8830 - val_loss: 14.9876 - val_att_decoder_loss: 15.8194 - val_ctc_decoder_loss: 12.4920 - val_att_decoder_acc: 0.9627\nEpoch 65/100\n19/19 [==============================] - 14s 745ms/step - loss: 16.1198 - att_decoder_loss: 17.2686 - ctc_decoder_loss: 12.6731 - att_decoder_acc: 0.8857 - val_loss: 14.8868 - val_att_decoder_loss: 15.8072 - val_ctc_decoder_loss: 12.1256 - val_att_decoder_acc: 0.9608\nEpoch 66/100\n19/19 [==============================] - 13s 701ms/step - loss: 16.0236 - att_decoder_loss: 17.2364 - ctc_decoder_loss: 12.3850 - att_decoder_acc: 0.8858 - val_loss: 14.9064 - val_att_decoder_loss: 15.7878 - val_ctc_decoder_loss: 12.2622 - val_att_decoder_acc: 0.9595\nEpoch 67/100\n19/19 [==============================] - 14s 755ms/step - loss: 16.0556 - att_decoder_loss: 17.3424 - ctc_decoder_loss: 12.1953 - att_decoder_acc: 0.8941 - val_loss: 14.7968 - val_att_decoder_loss: 15.8189 - val_ctc_decoder_loss: 11.7306 - val_att_decoder_acc: 0.9587\nEpoch 68/100\n19/19 [==============================] - 13s 702ms/step - loss: 15.9084 - att_decoder_loss: 17.2924 - ctc_decoder_loss: 11.7566 - att_decoder_acc: 0.8962 - val_loss: 14.8676 - val_att_decoder_loss: 15.8985 - val_ctc_decoder_loss: 11.7749 - val_att_decoder_acc: 0.9531\nEpoch 69/100\n19/19 [==============================] - 14s 753ms/step - loss: 15.6170 - att_decoder_loss: 17.0869 - ctc_decoder_loss: 11.2071 - att_decoder_acc: 0.9000 - val_loss: 14.4732 - val_att_decoder_loss: 15.6695 - val_ctc_decoder_loss: 10.8843 - val_att_decoder_acc: 0.9636\nEpoch 70/100\n19/19 [==============================] - 13s 703ms/step - loss: 15.3266 - att_decoder_loss: 16.8684 - ctc_decoder_loss: 10.7013 - att_decoder_acc: 0.9040 - val_loss: 14.5160 - val_att_decoder_loss: 15.7674 - val_ctc_decoder_loss: 10.7619 - val_att_decoder_acc: 0.9618\nEpoch 71/100\n19/19 [==============================] - 14s 758ms/step - loss: 15.3373 - att_decoder_loss: 16.8618 - ctc_decoder_loss: 10.7638 - att_decoder_acc: 0.9064 - val_loss: 14.4495 - val_att_decoder_loss: 15.6922 - val_ctc_decoder_loss: 10.7213 - val_att_decoder_acc: 0.9617\nEpoch 72/100\n19/19 [==============================] - 13s 701ms/step - loss: 15.0893 - att_decoder_loss: 16.7832 - ctc_decoder_loss: 10.0076 - att_decoder_acc: 0.9166 - val_loss: 14.5249 - val_att_decoder_loss: 15.6443 - val_ctc_decoder_loss: 11.1666 - val_att_decoder_acc: 0.9615\nEpoch 73/100\n19/19 [==============================] - 14s 757ms/step - loss: 14.8892 - att_decoder_loss: 16.6129 - ctc_decoder_loss: 9.7181 - att_decoder_acc: 0.9154 - val_loss: 14.4407 - val_att_decoder_loss: 15.6767 - val_ctc_decoder_loss: 10.7328 - val_att_decoder_acc: 0.9619\nEpoch 74/100\n19/19 [==============================] - 13s 714ms/step - loss: 14.8494 - att_decoder_loss: 16.6759 - ctc_decoder_loss: 9.3701 - att_decoder_acc: 0.9165 - val_loss: 14.4433 - val_att_decoder_loss: 15.6437 - val_ctc_decoder_loss: 10.8423 - val_att_decoder_acc: 0.9600\nEpoch 75/100\n19/19 [==============================] - 14s 755ms/step - loss: 14.8035 - att_decoder_loss: 16.6562 - ctc_decoder_loss: 9.2455 - att_decoder_acc: 0.9165 - val_loss: 14.2697 - val_att_decoder_loss: 15.6765 - val_ctc_decoder_loss: 10.0492 - val_att_decoder_acc: 0.9609\nEpoch 76/100\n19/19 [==============================] - 13s 712ms/step - loss: 14.8258 - att_decoder_loss: 16.6329 - ctc_decoder_loss: 9.4044 - att_decoder_acc: 0.9246 - val_loss: 14.2982 - val_att_decoder_loss: 15.6147 - val_ctc_decoder_loss: 10.3489 - val_att_decoder_acc: 0.9637\nEpoch 77/100\n19/19 [==============================] - 13s 706ms/step - loss: 14.7848 - att_decoder_loss: 16.6037 - ctc_decoder_loss: 9.3279 - att_decoder_acc: 0.9213 - val_loss: 14.3194 - val_att_decoder_loss: 15.6503 - val_ctc_decoder_loss: 10.3269 - val_att_decoder_acc: 0.9620\nEpoch 78/100\n19/19 [==============================] - 16s 872ms/step - loss: 14.4759 - att_decoder_loss: 16.4141 - ctc_decoder_loss: 8.6614 - att_decoder_acc: 0.9249 - val_loss: 14.3081 - val_att_decoder_loss: 15.6050 - val_ctc_decoder_loss: 10.4174 - val_att_decoder_acc: 0.9640\nEpoch 79/100\n19/19 [==============================] - 13s 712ms/step - loss: 14.4226 - att_decoder_loss: 16.4212 - ctc_decoder_loss: 8.4268 - att_decoder_acc: 0.9238 - val_loss: 14.4567 - val_att_decoder_loss: 15.7408 - val_ctc_decoder_loss: 10.6043 - val_att_decoder_acc: 0.9574\nEpoch 80/100\n19/19 [==============================] - 14s 762ms/step - loss: 14.4139 - att_decoder_loss: 16.5382 - ctc_decoder_loss: 8.0412 - att_decoder_acc: 0.9337 - val_loss: 14.2594 - val_att_decoder_loss: 15.6583 - val_ctc_decoder_loss: 10.0624 - val_att_decoder_acc: 0.9632\nEpoch 81/100\n19/19 [==============================] - 14s 778ms/step - loss: 14.2670 - att_decoder_loss: 16.2972 - ctc_decoder_loss: 8.1764 - att_decoder_acc: 0.9358 - val_loss: 14.1936 - val_att_decoder_loss: 15.5640 - val_ctc_decoder_loss: 10.0823 - val_att_decoder_acc: 0.9642\nEpoch 82/100\n19/19 [==============================] - 13s 722ms/step - loss: 14.3984 - att_decoder_loss: 16.4835 - ctc_decoder_loss: 8.1431 - att_decoder_acc: 0.9360 - val_loss: 14.2876 - val_att_decoder_loss: 15.6422 - val_ctc_decoder_loss: 10.2238 - val_att_decoder_acc: 0.9615\nEpoch 83/100\n19/19 [==============================] - 14s 765ms/step - loss: 14.1596 - att_decoder_loss: 16.2258 - ctc_decoder_loss: 7.9607 - att_decoder_acc: 0.9350 - val_loss: 14.1775 - val_att_decoder_loss: 15.5751 - val_ctc_decoder_loss: 9.9849 - val_att_decoder_acc: 0.9621\nEpoch 84/100\n19/19 [==============================] - 13s 709ms/step - loss: 13.9879 - att_decoder_loss: 16.1020 - ctc_decoder_loss: 7.6457 - att_decoder_acc: 0.9337 - val_loss: 14.1857 - val_att_decoder_loss: 15.5844 - val_ctc_decoder_loss: 9.9898 - val_att_decoder_acc: 0.9639\nEpoch 85/100\n19/19 [==============================] - 13s 718ms/step - loss: 14.1666 - att_decoder_loss: 16.3527 - ctc_decoder_loss: 7.6082 - att_decoder_acc: 0.9390 - val_loss: 14.1927 - val_att_decoder_loss: 15.5878 - val_ctc_decoder_loss: 10.0075 - val_att_decoder_acc: 0.9627\nEpoch 86/100\n19/19 [==============================] - 13s 738ms/step - loss: 14.0993 - att_decoder_loss: 16.2930 - ctc_decoder_loss: 7.5180 - att_decoder_acc: 0.9398 - val_loss: 14.1328 - val_att_decoder_loss: 15.5533 - val_ctc_decoder_loss: 9.8712 - val_att_decoder_acc: 0.9641\nEpoch 87/100\n19/19 [==============================] - 13s 706ms/step - loss: 14.0309 - att_decoder_loss: 16.1853 - ctc_decoder_loss: 7.5676 - att_decoder_acc: 0.9372 - val_loss: 14.1774 - val_att_decoder_loss: 15.5953 - val_ctc_decoder_loss: 9.9238 - val_att_decoder_acc: 0.9631\nEpoch 88/100\n19/19 [==============================] - 13s 710ms/step - loss: 14.1209 - att_decoder_loss: 16.4224 - ctc_decoder_loss: 7.2164 - att_decoder_acc: 0.9416 - val_loss: 14.1825 - val_att_decoder_loss: 15.5978 - val_ctc_decoder_loss: 9.9367 - val_att_decoder_acc: 0.9621\nEpoch 89/100\n19/19 [==============================] - 13s 707ms/step - loss: 14.0124 - att_decoder_loss: 16.2111 - ctc_decoder_loss: 7.4160 - att_decoder_acc: 0.9398 - val_loss: 14.1709 - val_att_decoder_loss: 15.5866 - val_ctc_decoder_loss: 9.9236 - val_att_decoder_acc: 0.9632\nEpoch 90/100\n19/19 [==============================] - 13s 709ms/step - loss: 14.0064 - att_decoder_loss: 16.2022 - ctc_decoder_loss: 7.4193 - att_decoder_acc: 0.9378 - val_loss: 14.1737 - val_att_decoder_loss: 15.5828 - val_ctc_decoder_loss: 9.9462 - val_att_decoder_acc: 0.9612\nEpoch 91/100\n19/19 [==============================] - 13s 721ms/step - loss: 13.7825 - att_decoder_loss: 16.0525 - ctc_decoder_loss: 6.9727 - att_decoder_acc: 0.9456 - val_loss: 14.1396 - val_att_decoder_loss: 15.5628 - val_ctc_decoder_loss: 9.8700 - val_att_decoder_acc: 0.9627\nEpoch 92/100\n19/19 [==============================] - 13s 717ms/step - loss: 13.9114 - att_decoder_loss: 16.1677 - ctc_decoder_loss: 7.1423 - att_decoder_acc: 0.9467 - val_loss: 14.1605 - val_att_decoder_loss: 15.5462 - val_ctc_decoder_loss: 10.0036 - val_att_decoder_acc: 0.9625\nEpoch 93/100\n19/19 [==============================] - 13s 718ms/step - loss: 13.8069 - att_decoder_loss: 16.0332 - ctc_decoder_loss: 7.1280 - att_decoder_acc: 0.9447 - val_loss: 14.1479 - val_att_decoder_loss: 15.5482 - val_ctc_decoder_loss: 9.9469 - val_att_decoder_acc: 0.9617\nEpoch 94/100\n19/19 [==============================] - 13s 714ms/step - loss: 13.7843 - att_decoder_loss: 16.1026 - ctc_decoder_loss: 6.8294 - att_decoder_acc: 0.9445 - val_loss: 14.1574 - val_att_decoder_loss: 15.5532 - val_ctc_decoder_loss: 9.9698 - val_att_decoder_acc: 0.9616\nEpoch 95/100\n19/19 [==============================] - 14s 762ms/step - loss: 13.9134 - att_decoder_loss: 16.1450 - ctc_decoder_loss: 7.2188 - att_decoder_acc: 0.9406 - val_loss: 14.1317 - val_att_decoder_loss: 15.5403 - val_ctc_decoder_loss: 9.9057 - val_att_decoder_acc: 0.9619\nEpoch 96/100\n19/19 [==============================] - 13s 744ms/step - loss: 13.8169 - att_decoder_loss: 16.1277 - ctc_decoder_loss: 6.8844 - att_decoder_acc: 0.9457 - val_loss: 14.1285 - val_att_decoder_loss: 15.5427 - val_ctc_decoder_loss: 9.8857 - val_att_decoder_acc: 0.9618\nEpoch 97/100\n19/19 [==============================] - 13s 712ms/step - loss: 13.8913 - att_decoder_loss: 16.2448 - ctc_decoder_loss: 6.8306 - att_decoder_acc: 0.9434 - val_loss: 14.1337 - val_att_decoder_loss: 15.5520 - val_ctc_decoder_loss: 9.8788 - val_att_decoder_acc: 0.9616\nEpoch 98/100\n19/19 [==============================] - 14s 755ms/step - loss: 13.7927 - att_decoder_loss: 16.0214 - ctc_decoder_loss: 7.1064 - att_decoder_acc: 0.9399 - val_loss: 14.1262 - val_att_decoder_loss: 15.5427 - val_ctc_decoder_loss: 9.8766 - val_att_decoder_acc: 0.9622\nEpoch 99/100\n19/19 [==============================] - 14s 754ms/step - loss: 13.8443 - att_decoder_loss: 16.0941 - ctc_decoder_loss: 7.0948 - att_decoder_acc: 0.9425 - val_loss: 14.1247 - val_att_decoder_loss: 15.5376 - val_ctc_decoder_loss: 9.8858 - val_att_decoder_acc: 0.9621\nEpoch 100/100\n19/19 [==============================] - 13s 715ms/step - loss: 13.7787 - att_decoder_loss: 16.0969 - ctc_decoder_loss: 6.8241 - att_decoder_acc: 0.9458 - val_loss: 14.1267 - val_att_decoder_loss: 15.5412 - val_ctc_decoder_loss: 9.8830 - val_att_decoder_acc: 0.9618\nATTENTION EVAL\n","output_type":"stream"},{"name":"stderr","text":"3it [06:38, 132.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"Target    : breach\nPrediction: breach\n----------------------------------------------------------------------------------------------------\nTarget    : indecent\nPrediction: indecent\n----------------------------------------------------------------------------------------------------\nTarget    : embolden\nPrediction: embolden\n----------------------------------------------------------------------------------------------------\nTarget    : uptown\nPrediction: uptown\n----------------------------------------------------------------------------------------------------\nTarget    : burridge\nPrediction: burridge\n----------------------------------------------------------------------------------------------------\nScore: 0.8480\nmean_dist: 1.1915\n\nCTC EVAL\n","output_type":"stream"},{"name":"stderr","text":"3it [05:23, 107.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"Target    : decal\nPrediction: dcal\n----------------------------------------------------------------------------------------------------\nTarget    : additive\nPrediction: additive\n----------------------------------------------------------------------------------------------------\nTarget    : zoned\nPrediction: zoned\n----------------------------------------------------------------------------------------------------\nTarget    : iroquois\nPrediction: roquos\n----------------------------------------------------------------------------------------------------\nTarget    : unsure\nPrediction: unsure\n----------------------------------------------------------------------------------------------------\nScore: 0.7668\nmean_dist: 1.8284\n","output_type":"stream"}]}]}